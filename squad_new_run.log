/home/nis_local/jojimonv/tfv2/lib/python3.7/site-packages/absl/flags/_validators.py:359: UserWarning: Flag --model_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!
  'command line!' % flag_name)
2020-02-24 03:47:31.534534: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
W0224 03:47:31.535575 140051752298304 cross_device_ops.py:1258] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
I0224 03:47:31.535907 140051752298304 mirrored_strategy.py:502] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)
I0224 03:47:31.536232 140051752298304 run_squad.py:231] Training using customized training loop with distribution strategy.
2020-02-24 03:47:35.016073: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler" was not an Input tensor, it was generated by layer input_mask.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_mask:0
W0224 03:47:36.482812 140051752298304 network.py:1344] BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler" was not an Input tensor, it was generated by layer input_mask.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_mask:0
WARNING:tensorflow:BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler" was not an Input tensor, it was generated by layer input_type_ids.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_type_ids:0
W0224 03:47:36.482941 140051752298304 network.py:1344] BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler" was not an Input tensor, it was generated by layer input_type_ids.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_type_ids:0
I0224 03:47:36.489112 140051752298304 model_training_utils.py:213] Checkpoint file /nfs/site/home/jojimonv/Models/TF2.0/DATA/keras_bert/uncased_L-24_H-1024_A-16/bert_model.ckpt found and restoring from initial checkpoint for core model.
I0224 03:47:37.395134 140051752298304 model_training_utils.py:216] Loading from checkpoint file completed
WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss.
W0224 03:47:40.456183 140038264293120 optimizer_v2.py:1096] Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss.
W0224 03:47:45.438178 140038264293120 optimizer_v2.py:1096] Gradients do not exist for variables ['pooler_transform/kernel:0', 'pooler_transform/bias:0'] when minimizing the loss.
I0224 03:54:54.513528 140051752298304 model_training_utils.py:391] Train Step: 10/5540  / loss = 5.954440116882324
I0224 04:01:13.009914 140051752298304 model_training_utils.py:391] Train Step: 20/5540  / loss = 5.902818202972412
I0224 04:07:20.044665 140051752298304 model_training_utils.py:391] Train Step: 30/5540  / loss = 5.777232646942139
I0224 04:13:23.884136 140051752298304 model_training_utils.py:391] Train Step: 40/5540  / loss = 5.582067012786865
I0224 04:19:31.351339 140051752298304 model_training_utils.py:391] Train Step: 50/5540  / loss = 5.24928092956543
I0224 04:25:35.255825 140051752298304 model_training_utils.py:391] Train Step: 60/5540  / loss = 4.937260627746582
I0224 04:31:39.498275 140051752298304 model_training_utils.py:391] Train Step: 70/5540  / loss = 4.425637722015381
I0224 04:37:44.791432 140051752298304 model_training_utils.py:391] Train Step: 80/5540  / loss = 4.459280967712402
I0224 04:43:48.950842 140051752298304 model_training_utils.py:391] Train Step: 90/5540  / loss = 3.7221744060516357
I0224 04:49:53.718082 140051752298304 model_training_utils.py:391] Train Step: 100/5540  / loss = 3.3734962940216064
I0224 04:56:01.125396 140051752298304 model_training_utils.py:391] Train Step: 110/5540  / loss = 2.98715877532959
I0224 05:02:05.026459 140051752298304 model_training_utils.py:391] Train Step: 120/5540  / loss = 2.357595920562744
I0224 05:08:11.892379 140051752298304 model_training_utils.py:391] Train Step: 130/5540  / loss = 1.8479602336883545
I0224 05:14:16.213047 140051752298304 model_training_utils.py:391] Train Step: 140/5540  / loss = 1.5053781270980835
I0224 05:20:22.010826 140051752298304 model_training_utils.py:391] Train Step: 150/5540  / loss = 1.5922447443008423
I0224 05:26:24.815132 140051752298304 model_training_utils.py:391] Train Step: 160/5540  / loss = 2.1283535957336426
I0224 05:32:26.451524 140051752298304 model_training_utils.py:391] Train Step: 170/5540  / loss = 2.032691717147827
I0224 05:38:28.020833 140051752298304 model_training_utils.py:391] Train Step: 180/5540  / loss = 1.7095775604248047
I0224 05:44:31.395105 140051752298304 model_training_utils.py:391] Train Step: 190/5540  / loss = 1.5789657831192017
I0224 05:50:34.663851 140051752298304 model_training_utils.py:391] Train Step: 200/5540  / loss = 1.5667264461517334
I0224 05:56:35.983554 140051752298304 model_training_utils.py:391] Train Step: 210/5540  / loss = 1.781205415725708
I0224 06:02:38.574638 140051752298304 model_training_utils.py:391] Train Step: 220/5540  / loss = 1.6402473449707031
I0224 06:08:39.739443 140051752298304 model_training_utils.py:391] Train Step: 230/5540  / loss = 1.4858921766281128
I0224 06:14:41.519446 140051752298304 model_training_utils.py:391] Train Step: 240/5540  / loss = 0.9865449070930481
I0224 06:20:41.273057 140051752298304 model_training_utils.py:391] Train Step: 250/5540  / loss = 1.5112065076828003
I0224 06:26:45.398302 140051752298304 model_training_utils.py:391] Train Step: 260/5540  / loss = 1.600956678390503
I0224 06:32:46.170886 140051752298304 model_training_utils.py:391] Train Step: 270/5540  / loss = 1.3660171031951904
I0224 06:38:47.696478 140051752298304 model_training_utils.py:391] Train Step: 280/5540  / loss = 2.162440776824951
I0224 06:44:48.915539 140051752298304 model_training_utils.py:391] Train Step: 290/5540  / loss = 1.5505001544952393
I0224 06:50:49.047941 140051752298304 model_training_utils.py:391] Train Step: 300/5540  / loss = 1.1941354274749756
I0224 06:56:53.325614 140051752298304 model_training_utils.py:391] Train Step: 310/5540  / loss = 1.1279370784759521
I0224 07:02:53.232901 140051752298304 model_training_utils.py:391] Train Step: 320/5540  / loss = 0.9810357093811035
I0224 07:08:54.871235 140051752298304 model_training_utils.py:391] Train Step: 330/5540  / loss = 1.5378057956695557
I0224 07:14:56.337161 140051752298304 model_training_utils.py:391] Train Step: 340/5540  / loss = 1.443159818649292
I0224 07:20:58.759650 140051752298304 model_training_utils.py:391] Train Step: 350/5540  / loss = 1.410796046257019
I0224 07:26:59.462833 140051752298304 model_training_utils.py:391] Train Step: 360/5540  / loss = 1.0724680423736572
I0224 07:32:58.986584 140051752298304 model_training_utils.py:391] Train Step: 370/5540  / loss = 0.9125250577926636
I0224 07:38:58.552849 140051752298304 model_training_utils.py:391] Train Step: 380/5540  / loss = 1.0055429935455322
I0224 07:44:57.337763 140051752298304 model_training_utils.py:391] Train Step: 390/5540  / loss = 1.0462586879730225
I0224 07:50:55.205895 140051752298304 model_training_utils.py:391] Train Step: 400/5540  / loss = 1.0017213821411133
I0224 07:56:55.089707 140051752298304 model_training_utils.py:391] Train Step: 410/5540  / loss = 1.0271146297454834
I0224 08:02:56.715126 140051752298304 model_training_utils.py:391] Train Step: 420/5540  / loss = 1.237666368484497
I0224 08:08:55.158201 140051752298304 model_training_utils.py:391] Train Step: 430/5540  / loss = 0.958916962146759
I0224 08:14:49.253474 140051752298304 model_training_utils.py:391] Train Step: 440/5540  / loss = 1.2474031448364258
I0224 08:20:47.087160 140051752298304 model_training_utils.py:391] Train Step: 450/5540  / loss = 1.1555731296539307
I0224 08:26:45.705667 140051752298304 model_training_utils.py:391] Train Step: 460/5540  / loss = 1.1705535650253296
I0224 08:32:44.878658 140051752298304 model_training_utils.py:391] Train Step: 470/5540  / loss = 1.2165876626968384
I0224 08:38:40.932991 140051752298304 model_training_utils.py:391] Train Step: 480/5540  / loss = 1.1286811828613281
I0224 08:44:37.517989 140051752298304 model_training_utils.py:391] Train Step: 490/5540  / loss = 1.3068548440933228
I0224 08:50:34.618608 140051752298304 model_training_utils.py:391] Train Step: 500/5540  / loss = 1.2378498315811157
I0224 08:56:33.359477 140051752298304 model_training_utils.py:391] Train Step: 510/5540  / loss = 1.2570059299468994
I0224 09:02:30.610949 140051752298304 model_training_utils.py:391] Train Step: 520/5540  / loss = 1.0221389532089233
I0224 09:08:24.730874 140051752298304 model_training_utils.py:391] Train Step: 530/5540  / loss = 0.8784538507461548
I0224 09:14:18.433189 140051752298304 model_training_utils.py:391] Train Step: 540/5540  / loss = 1.2640336751937866
I0224 09:20:15.429949 140051752298304 model_training_utils.py:391] Train Step: 550/5540  / loss = 1.204967737197876
I0224 09:26:11.361240 140051752298304 model_training_utils.py:391] Train Step: 560/5540  / loss = 1.1898720264434814
I0224 09:32:07.485704 140051752298304 model_training_utils.py:391] Train Step: 570/5540  / loss = 1.0477749109268188
I0224 09:38:02.928669 140051752298304 model_training_utils.py:391] Train Step: 580/5540  / loss = 1.120145559310913
I0224 09:43:56.953798 140051752298304 model_training_utils.py:391] Train Step: 590/5540  / loss = 1.0242189168930054
I0224 09:49:53.470993 140051752298304 model_training_utils.py:391] Train Step: 600/5540  / loss = 0.691824197769165
I0224 09:55:49.737330 140051752298304 model_training_utils.py:391] Train Step: 610/5540  / loss = 0.833443820476532
I0224 10:01:45.423677 140051752298304 model_training_utils.py:391] Train Step: 620/5540  / loss = 1.1429381370544434
I0224 10:07:39.393741 140051752298304 model_training_utils.py:391] Train Step: 630/5540  / loss = 1.0903868675231934
I0224 10:13:36.004430 140051752298304 model_training_utils.py:391] Train Step: 640/5540  / loss = 1.2896134853363037
I0224 10:19:29.977579 140051752298304 model_training_utils.py:391] Train Step: 650/5540  / loss = 1.4379910230636597
I0224 10:25:27.318182 140051752298304 model_training_utils.py:391] Train Step: 660/5540  / loss = 0.8181281089782715
I0224 10:31:22.560316 140051752298304 model_training_utils.py:391] Train Step: 670/5540  / loss = 1.0044677257537842
I0224 10:37:18.376596 140051752298304 model_training_utils.py:391] Train Step: 680/5540  / loss = 0.7064394354820251
I0224 10:43:13.013643 140051752298304 model_training_utils.py:391] Train Step: 690/5540  / loss = 1.0643984079360962
I0224 10:49:09.712181 140051752298304 model_training_utils.py:391] Train Step: 700/5540  / loss = 0.9728156924247742
I0224 10:55:04.341874 140051752298304 model_training_utils.py:391] Train Step: 710/5540  / loss = 1.006978988647461
I0224 11:00:59.842630 140051752298304 model_training_utils.py:391] Train Step: 720/5540  / loss = 1.1019294261932373
I0224 11:06:56.015338 140051752298304 model_training_utils.py:391] Train Step: 730/5540  / loss = 0.9246166348457336
I0224 11:12:50.048767 140051752298304 model_training_utils.py:391] Train Step: 740/5540  / loss = 0.7554144263267517
I0224 11:18:46.405429 140051752298304 model_training_utils.py:391] Train Step: 750/5540  / loss = 0.7388020157814026
I0224 11:24:41.062537 140051752298304 model_training_utils.py:391] Train Step: 760/5540  / loss = 1.4369128942489624
I0224 11:30:34.786885 140051752298304 model_training_utils.py:391] Train Step: 770/5540  / loss = 1.2716721296310425
I0224 11:36:29.056410 140051752298304 model_training_utils.py:391] Train Step: 780/5540  / loss = 0.9384427070617676
I0224 11:42:25.407101 140051752298304 model_training_utils.py:391] Train Step: 790/5540  / loss = 1.0805413722991943
I0224 11:48:22.821388 140051752298304 model_training_utils.py:391] Train Step: 800/5540  / loss = 1.0234624147415161
I0224 11:54:18.474376 140051752298304 model_training_utils.py:391] Train Step: 810/5540  / loss = 1.0345369577407837
I0224 12:00:15.935501 140051752298304 model_training_utils.py:391] Train Step: 820/5540  / loss = 0.8769731521606445
I0224 12:06:15.270433 140051752298304 model_training_utils.py:391] Train Step: 830/5540  / loss = 0.9022009968757629
I0224 12:12:11.909459 140051752298304 model_training_utils.py:391] Train Step: 840/5540  / loss = 0.8733510971069336
I0224 12:18:06.279086 140051752298304 model_training_utils.py:391] Train Step: 850/5540  / loss = 1.2406123876571655
I0224 12:24:02.085711 140051752298304 model_training_utils.py:391] Train Step: 860/5540  / loss = 1.1384129524230957
I0224 12:29:55.485530 140051752298304 model_training_utils.py:391] Train Step: 870/5540  / loss = 1.100073218345642
I0224 12:35:52.944211 140051752298304 model_training_utils.py:391] Train Step: 880/5540  / loss = 1.0169775485992432
I0224 12:41:44.448983 140051752298304 model_training_utils.py:391] Train Step: 890/5540  / loss = 1.0206080675125122
I0224 12:47:42.505426 140051752298304 model_training_utils.py:391] Train Step: 900/5540  / loss = 0.7209702730178833
I0224 12:53:36.117488 140051752298304 model_training_utils.py:391] Train Step: 910/5540  / loss = 0.9104828834533691
I0224 12:59:30.802206 140051752298304 model_training_utils.py:391] Train Step: 920/5540  / loss = 0.9504119753837585
I0224 13:05:26.956941 140051752298304 model_training_utils.py:391] Train Step: 930/5540  / loss = 1.295994758605957
I0224 13:11:23.688453 140051752298304 model_training_utils.py:391] Train Step: 940/5540  / loss = 1.266041874885559
I0224 13:17:16.399386 140051752298304 model_training_utils.py:391] Train Step: 950/5540  / loss = 1.1521813869476318
I0224 13:23:10.661129 140051752298304 model_training_utils.py:391] Train Step: 960/5540  / loss = 1.2644561529159546
I0224 13:29:06.169832 140051752298304 model_training_utils.py:391] Train Step: 970/5540  / loss = 1.362703561782837
I0224 13:34:58.924385 140051752298304 model_training_utils.py:391] Train Step: 980/5540  / loss = 1.2497169971466064
I0224 13:40:53.475955 140051752298304 model_training_utils.py:391] Train Step: 990/5540  / loss = 1.0347826480865479
I0224 13:46:49.472228 140051752298304 model_training_utils.py:391] Train Step: 1000/5540  / loss = 0.6023510694503784
I0224 13:52:45.137204 140051752298304 model_training_utils.py:391] Train Step: 1010/5540  / loss = 0.7565513849258423
I0224 13:58:39.708209 140051752298304 model_training_utils.py:391] Train Step: 1020/5540  / loss = 0.9959855079650879
I0224 14:04:37.558763 140051752298304 model_training_utils.py:391] Train Step: 1030/5540  / loss = 0.8897992968559265
I0224 14:10:31.552388 140051752298304 model_training_utils.py:391] Train Step: 1040/5540  / loss = 0.7123781442642212
I0224 14:16:25.251709 140051752298304 model_training_utils.py:391] Train Step: 1050/5540  / loss = 0.7859808206558228
I0224 14:22:19.990713 140051752298304 model_training_utils.py:391] Train Step: 1060/5540  / loss = 0.9753943681716919
I0224 14:28:15.834865 140051752298304 model_training_utils.py:391] Train Step: 1070/5540  / loss = 0.8089939951896667
I0224 14:34:10.880624 140051752298304 model_training_utils.py:391] Train Step: 1080/5540  / loss = 1.0431832075119019
I0224 14:40:06.932922 140051752298304 model_training_utils.py:391] Train Step: 1090/5540  / loss = 1.3395161628723145
I0224 14:46:00.021437 140051752298304 model_training_utils.py:391] Train Step: 1100/5540  / loss = 2.206608295440674
I0224 14:51:51.437164 140051752298304 model_training_utils.py:391] Train Step: 1110/5540  / loss = 0.9980368614196777
I0224 14:57:47.726565 140051752298304 model_training_utils.py:391] Train Step: 1120/5540  / loss = 1.0800540447235107
I0224 15:03:41.631936 140051752298304 model_training_utils.py:391] Train Step: 1130/5540  / loss = 1.8598377704620361
I0224 15:09:37.612670 140051752298304 model_training_utils.py:391] Train Step: 1140/5540  / loss = 1.5034910440444946
I0224 15:15:30.843221 140051752298304 model_training_utils.py:391] Train Step: 1150/5540  / loss = 2.257901668548584
I0224 15:21:24.246541 140051752298304 model_training_utils.py:391] Train Step: 1160/5540  / loss = 1.0170817375183105
I0224 15:27:18.722987 140051752298304 model_training_utils.py:391] Train Step: 1170/5540  / loss = 0.9222105145454407
I0224 15:33:14.841811 140051752298304 model_training_utils.py:391] Train Step: 1180/5540  / loss = 1.0386402606964111
I0224 15:39:09.507611 140051752298304 model_training_utils.py:391] Train Step: 1190/5540  / loss = 1.2117371559143066
I0224 15:45:05.585117 140051752298304 model_training_utils.py:391] Train Step: 1200/5540  / loss = 1.0812859535217285
I0224 15:51:00.810458 140051752298304 model_training_utils.py:391] Train Step: 1210/5540  / loss = 0.8313348889350891
I0224 15:56:52.176695 140051752298304 model_training_utils.py:391] Train Step: 1220/5540  / loss = 0.6684286594390869
I0224 16:02:48.316534 140051752298304 model_training_utils.py:391] Train Step: 1230/5540  / loss = 0.6980130076408386
I0224 16:08:44.983826 140051752298304 model_training_utils.py:391] Train Step: 1240/5540  / loss = 0.5493770837783813
I0224 16:14:40.590616 140051752298304 model_training_utils.py:391] Train Step: 1250/5540  / loss = 1.6080834865570068
I0224 16:20:38.556438 140051752298304 model_training_utils.py:391] Train Step: 1260/5540  / loss = 0.8044455647468567
I0224 16:26:34.357877 140051752298304 model_training_utils.py:391] Train Step: 1270/5540  / loss = 1.887778878211975
I0224 16:32:25.898249 140051752298304 model_training_utils.py:391] Train Step: 1280/5540  / loss = 1.2628037929534912
I0224 16:38:19.837948 140051752298304 model_training_utils.py:391] Train Step: 1290/5540  / loss = 1.036850929260254
I0224 16:44:13.804117 140051752298304 model_training_utils.py:391] Train Step: 1300/5540  / loss = 1.1103730201721191
I0224 16:50:07.622882 140051752298304 model_training_utils.py:391] Train Step: 1310/5540  / loss = 1.160526156425476
I0224 16:56:05.010099 140051752298304 model_training_utils.py:391] Train Step: 1320/5540  / loss = 1.3845523595809937
I0224 17:01:58.585305 140051752298304 model_training_utils.py:391] Train Step: 1330/5540  / loss = 1.089490294456482
I0224 17:07:51.928955 140051752298304 model_training_utils.py:391] Train Step: 1340/5540  / loss = 0.8950111269950867
I0224 17:13:45.773791 140051752298304 model_training_utils.py:391] Train Step: 1350/5540  / loss = 1.0303059816360474
I0224 17:19:39.541696 140051752298304 model_training_utils.py:391] Train Step: 1360/5540  / loss = 1.740404486656189
I0224 17:25:30.963943 140051752298304 model_training_utils.py:391] Train Step: 1370/5540  / loss = 1.4146769046783447
I0224 17:31:22.230145 140051752298304 model_training_utils.py:391] Train Step: 1380/5540  / loss = 1.164489984512329
I0224 17:37:14.608525 140051752298304 model_training_utils.py:391] Train Step: 1390/5540  / loss = 0.9759954214096069
I0224 17:43:11.677425 140051752298304 model_training_utils.py:391] Train Step: 1400/5540  / loss = 1.1702754497528076
I0224 17:49:05.115613 140051752298304 model_training_utils.py:391] Train Step: 1410/5540  / loss = 0.9023835062980652
I0224 17:54:57.547199 140051752298304 model_training_utils.py:391] Train Step: 1420/5540  / loss = 1.1797369718551636
I0224 18:00:50.119051 140051752298304 model_training_utils.py:391] Train Step: 1430/5540  / loss = 1.6803261041641235
I0224 18:06:41.391497 140051752298304 model_training_utils.py:391] Train Step: 1440/5540  / loss = 1.3580632209777832
I0224 18:12:34.177134 140051752298304 model_training_utils.py:391] Train Step: 1450/5540  / loss = 1.5632120370864868
I0224 18:18:28.917903 140051752298304 model_training_utils.py:391] Train Step: 1460/5540  / loss = 0.9349228739738464
I0224 18:24:20.447349 140051752298304 model_training_utils.py:391] Train Step: 1470/5540  / loss = 1.6962873935699463
I0224 18:30:15.814037 140051752298304 model_training_utils.py:391] Train Step: 1480/5540  / loss = 0.9795717000961304
I0224 18:36:09.101016 140051752298304 model_training_utils.py:391] Train Step: 1490/5540  / loss = 0.9018742442131042
I0224 18:42:02.101523 140051752298304 model_training_utils.py:391] Train Step: 1500/5540  / loss = 2.1274170875549316
I0224 18:47:55.264718 140051752298304 model_training_utils.py:391] Train Step: 1510/5540  / loss = 2.103895425796509
I0224 18:53:47.095092 140051752298304 model_training_utils.py:391] Train Step: 1520/5540  / loss = 1.2217614650726318
I0224 18:59:36.965068 140051752298304 model_training_utils.py:391] Train Step: 1530/5540  / loss = 1.009670615196228
I0224 19:05:29.672915 140051752298304 model_training_utils.py:391] Train Step: 1540/5540  / loss = 1.8176171779632568
I0224 19:11:24.069864 140051752298304 model_training_utils.py:391] Train Step: 1550/5540  / loss = 1.5797524452209473
I0224 19:17:15.239895 140051752298304 model_training_utils.py:391] Train Step: 1560/5540  / loss = 0.8975728154182434
I0224 19:23:08.152483 140051752298304 model_training_utils.py:391] Train Step: 1570/5540  / loss = 1.087957739830017
I0224 19:29:00.399548 140051752298304 model_training_utils.py:391] Train Step: 1580/5540  / loss = 0.9751326441764832
I0224 19:34:51.211051 140051752298304 model_training_utils.py:391] Train Step: 1590/5540  / loss = 0.8126228451728821
I0224 19:40:42.922501 140051752298304 model_training_utils.py:391] Train Step: 1600/5540  / loss = 1.004926323890686
I0224 19:46:33.844130 140051752298304 model_training_utils.py:391] Train Step: 1610/5540  / loss = 1.3425941467285156
I0224 19:52:28.776513 140051752298304 model_training_utils.py:391] Train Step: 1620/5540  / loss = 0.9591218829154968
I0224 19:58:18.960467 140051752298304 model_training_utils.py:391] Train Step: 1630/5540  / loss = 1.4591491222381592
I0224 20:04:09.122227 140051752298304 model_training_utils.py:391] Train Step: 1640/5540  / loss = 1.1171400547027588
I0224 20:10:00.681073 140051752298304 model_training_utils.py:391] Train Step: 1650/5540  / loss = 1.0193201303482056
I0224 20:15:54.601508 140051752298304 model_training_utils.py:391] Train Step: 1660/5540  / loss = 1.175366997718811
I0224 20:21:49.308406 140051752298304 model_training_utils.py:391] Train Step: 1670/5540  / loss = 1.3142904043197632
I0224 20:27:42.115763 140051752298304 model_training_utils.py:391] Train Step: 1680/5540  / loss = 1.4818508625030518
I0224 20:33:34.706529 140051752298304 model_training_utils.py:391] Train Step: 1690/5540  / loss = 1.6879603862762451
I0224 20:39:29.268083 140051752298304 model_training_utils.py:391] Train Step: 1700/5540  / loss = 1.481795072555542
I0224 20:45:19.396180 140051752298304 model_training_utils.py:391] Train Step: 1710/5540  / loss = 1.07223379611969
I0224 20:51:11.720901 140051752298304 model_training_utils.py:391] Train Step: 1720/5540  / loss = 1.2417665719985962
I0224 20:57:04.797429 140051752298304 model_training_utils.py:391] Train Step: 1730/5540  / loss = 1.3195037841796875
I0224 21:02:57.098180 140051752298304 model_training_utils.py:391] Train Step: 1740/5540  / loss = 1.052109956741333
I0224 21:08:49.456897 140051752298304 model_training_utils.py:391] Train Step: 1750/5540  / loss = 1.8700520992279053
I0224 21:14:42.381554 140051752298304 model_training_utils.py:391] Train Step: 1760/5540  / loss = 1.33621084690094
I0224 21:20:36.186682 140051752298304 model_training_utils.py:391] Train Step: 1770/5540  / loss = 0.8328941464424133
I0224 21:26:29.209969 140051752298304 model_training_utils.py:391] Train Step: 1780/5540  / loss = 1.2694636583328247
I0224 21:32:22.711117 140051752298304 model_training_utils.py:391] Train Step: 1790/5540  / loss = 1.2457125186920166
I0224 21:38:16.288918 140051752298304 model_training_utils.py:391] Train Step: 1800/5540  / loss = 1.2808682918548584
I0224 21:44:10.137628 140051752298304 model_training_utils.py:391] Train Step: 1810/5540  / loss = 1.6412832736968994
I0224 21:50:01.381330 140051752298304 model_training_utils.py:391] Train Step: 1820/5540  / loss = 2.024228572845459
I0224 21:55:52.855954 140051752298304 model_training_utils.py:391] Train Step: 1830/5540  / loss = 1.2433116436004639
I0224 22:01:45.644859 140051752298304 model_training_utils.py:391] Train Step: 1840/5540  / loss = 1.026136040687561
I0224 22:07:36.340445 140051752298304 model_training_utils.py:391] Train Step: 1850/5540  / loss = 1.201684832572937
I0224 22:13:28.513484 140051752298304 model_training_utils.py:391] Train Step: 1860/5540  / loss = 1.4523980617523193
I0224 22:19:19.772413 140051752298304 model_training_utils.py:391] Train Step: 1870/5540  / loss = 0.9337803721427917
I0224 22:25:15.319820 140051752298304 model_training_utils.py:391] Train Step: 1880/5540  / loss = 1.023431420326233
I0224 22:31:06.991712 140051752298304 model_training_utils.py:391] Train Step: 1890/5540  / loss = 0.8021372556686401
I0224 22:36:59.594243 140051752298304 model_training_utils.py:391] Train Step: 1900/5540  / loss = 1.2732943296432495
I0224 22:42:51.549103 140051752298304 model_training_utils.py:391] Train Step: 1910/5540  / loss = 0.7674950361251831
I0224 22:48:45.514423 140051752298304 model_training_utils.py:391] Train Step: 1920/5540  / loss = 0.6349643468856812
I0224 22:54:35.322458 140051752298304 model_training_utils.py:391] Train Step: 1930/5540  / loss = 0.9981403350830078
I0224 23:00:28.296803 140051752298304 model_training_utils.py:391] Train Step: 1940/5540  / loss = 0.8282853960990906
I0224 23:06:21.222804 140051752298304 model_training_utils.py:391] Train Step: 1950/5540  / loss = 0.9633083343505859
I0224 23:12:14.031866 140051752298304 model_training_utils.py:391] Train Step: 1960/5540  / loss = 1.0837223529815674
I0224 23:18:07.341411 140051752298304 model_training_utils.py:391] Train Step: 1970/5540  / loss = 1.0699039697647095
I0224 23:23:59.434960 140051752298304 model_training_utils.py:391] Train Step: 1980/5540  / loss = 1.1413617134094238
I0224 23:29:51.993548 140051752298304 model_training_utils.py:391] Train Step: 1990/5540  / loss = 1.2754250764846802
I0224 23:35:44.025146 140051752298304 model_training_utils.py:391] Train Step: 2000/5540  / loss = 1.0950289964675903
I0224 23:41:37.212860 140051752298304 model_training_utils.py:391] Train Step: 2010/5540  / loss = 1.248924970626831
I0224 23:47:31.638009 140051752298304 model_training_utils.py:391] Train Step: 2020/5540  / loss = 0.8564779162406921
I0224 23:53:21.601546 140051752298304 model_training_utils.py:391] Train Step: 2030/5540  / loss = 0.9379121661186218
I0224 23:59:15.163203 140051752298304 model_training_utils.py:391] Train Step: 2040/5540  / loss = 1.1939094066619873
I0225 00:05:07.921250 140051752298304 model_training_utils.py:391] Train Step: 2050/5540  / loss = 0.8883956670761108
I0225 00:11:02.702567 140051752298304 model_training_utils.py:391] Train Step: 2060/5540  / loss = 0.6781082153320312
I0225 00:16:53.443814 140051752298304 model_training_utils.py:391] Train Step: 2070/5540  / loss = 0.7343273758888245
I0225 00:22:44.436454 140051752298304 model_training_utils.py:391] Train Step: 2080/5540  / loss = 0.6655871272087097
I0225 00:28:37.726912 140051752298304 model_training_utils.py:391] Train Step: 2090/5540  / loss = 1.2045177221298218
I0225 00:34:30.558568 140051752298304 model_training_utils.py:391] Train Step: 2100/5540  / loss = 0.982104480266571
I0225 00:40:20.339134 140051752298304 model_training_utils.py:391] Train Step: 2110/5540  / loss = 1.3058668375015259
I0225 00:46:12.961905 140051752298304 model_training_utils.py:391] Train Step: 2120/5540  / loss = 0.8952056169509888
I0225 00:52:06.613908 140051752298304 model_training_utils.py:391] Train Step: 2130/5540  / loss = 1.0749094486236572
I0225 00:58:00.154996 140051752298304 model_training_utils.py:391] Train Step: 2140/5540  / loss = 0.7402933835983276
I0225 01:03:52.215016 140051752298304 model_training_utils.py:391] Train Step: 2150/5540  / loss = 0.9389122128486633
I0225 01:09:44.645423 140051752298304 model_training_utils.py:391] Train Step: 2160/5540  / loss = 1.2266486883163452
I0225 01:15:38.695195 140051752298304 model_training_utils.py:391] Train Step: 2170/5540  / loss = 1.3118828535079956
I0225 01:21:34.325081 140051752298304 model_training_utils.py:391] Train Step: 2180/5540  / loss = 1.029245138168335
I0225 01:27:25.798887 140051752298304 model_training_utils.py:391] Train Step: 2190/5540  / loss = 0.9673229455947876
I0225 01:33:18.049779 140051752298304 model_training_utils.py:391] Train Step: 2200/5540  / loss = 1.035548210144043
I0225 01:39:10.884514 140051752298304 model_training_utils.py:391] Train Step: 2210/5540  / loss = 1.0407005548477173
I0225 01:45:07.134544 140051752298304 model_training_utils.py:391] Train Step: 2220/5540  / loss = 1.542670726776123
I0225 01:50:59.346169 140051752298304 model_training_utils.py:391] Train Step: 2230/5540  / loss = 2.435138702392578
I0225 01:56:52.976495 140051752298304 model_training_utils.py:391] Train Step: 2240/5540  / loss = 1.6177701950073242
I0225 02:02:43.431606 140051752298304 model_training_utils.py:391] Train Step: 2250/5540  / loss = 0.7719693183898926
I0225 02:08:37.121738 140051752298304 model_training_utils.py:391] Train Step: 2260/5540  / loss = 0.679764986038208
I0225 02:14:26.946269 140051752298304 model_training_utils.py:391] Train Step: 2270/5540  / loss = 1.0329177379608154
I0225 02:20:18.226431 140051752298304 model_training_utils.py:391] Train Step: 2280/5540  / loss = 1.4222280979156494
I0225 02:26:07.860622 140051752298304 model_training_utils.py:391] Train Step: 2290/5540  / loss = 1.8013134002685547
I0225 02:32:01.311631 140051752298304 model_training_utils.py:391] Train Step: 2300/5540  / loss = 0.9058597683906555
I0225 02:37:52.910521 140051752298304 model_training_utils.py:391] Train Step: 2310/5540  / loss = 0.865293025970459
I0225 02:43:46.227626 140051752298304 model_training_utils.py:391] Train Step: 2320/5540  / loss = 1.3188918828964233
I0225 02:49:37.246752 140051752298304 model_training_utils.py:391] Train Step: 2330/5540  / loss = 1.0409523248672485
I0225 02:55:28.593592 140051752298304 model_training_utils.py:391] Train Step: 2340/5540  / loss = 2.3293070793151855
I0225 03:01:19.915722 140051752298304 model_training_utils.py:391] Train Step: 2350/5540  / loss = 1.226381540298462
I0225 03:07:13.398313 140051752298304 model_training_utils.py:391] Train Step: 2360/5540  / loss = 1.1830567121505737
I0225 03:13:05.048660 140051752298304 model_training_utils.py:391] Train Step: 2370/5540  / loss = 1.2493008375167847
I0225 03:18:57.373677 140051752298304 model_training_utils.py:391] Train Step: 2380/5540  / loss = 1.4133825302124023
I0225 03:24:49.046281 140051752298304 model_training_utils.py:391] Train Step: 2390/5540  / loss = 1.5342000722885132
I0225 03:30:42.894454 140051752298304 model_training_utils.py:391] Train Step: 2400/5540  / loss = 0.9798046946525574
I0225 03:36:34.312231 140051752298304 model_training_utils.py:391] Train Step: 2410/5540  / loss = 0.7996061444282532
I0225 03:42:27.493794 140051752298304 model_training_utils.py:391] Train Step: 2420/5540  / loss = 1.0722553730010986
I0225 03:48:19.142096 140051752298304 model_training_utils.py:391] Train Step: 2430/5540  / loss = 0.7555578947067261
I0225 03:54:11.981645 140051752298304 model_training_utils.py:391] Train Step: 2440/5540  / loss = 0.9435817003250122
I0225 04:00:05.386987 140051752298304 model_training_utils.py:391] Train Step: 2450/5540  / loss = 1.3411107063293457
I0225 04:06:00.162193 140051752298304 model_training_utils.py:391] Train Step: 2460/5540  / loss = 1.9081312417984009
I0225 04:11:51.472514 140051752298304 model_training_utils.py:391] Train Step: 2470/5540  / loss = 1.4359747171401978
I0225 04:17:41.995768 140051752298304 model_training_utils.py:391] Train Step: 2480/5540  / loss = 1.0460253953933716
I0225 04:23:35.301234 140051752298304 model_training_utils.py:391] Train Step: 2490/5540  / loss = 0.6935929656028748
I0225 04:29:26.499330 140051752298304 model_training_utils.py:391] Train Step: 2500/5540  / loss = 0.7356891632080078
I0225 04:35:19.610967 140051752298304 model_training_utils.py:391] Train Step: 2510/5540  / loss = 0.8887802958488464
I0225 04:41:11.538304 140051752298304 model_training_utils.py:391] Train Step: 2520/5540  / loss = 0.898210346698761
I0225 04:47:04.408107 140051752298304 model_training_utils.py:391] Train Step: 2530/5540  / loss = 0.9608318209648132
I0225 04:52:54.926541 140051752298304 model_training_utils.py:391] Train Step: 2540/5540  / loss = 1.067183494567871
I0225 04:58:46.747314 140051752298304 model_training_utils.py:391] Train Step: 2550/5540  / loss = 0.7914345860481262
I0225 05:04:39.337575 140051752298304 model_training_utils.py:391] Train Step: 2560/5540  / loss = 0.9717143177986145
I0225 05:10:32.875415 140051752298304 model_training_utils.py:391] Train Step: 2570/5540  / loss = 1.0503225326538086
I0225 05:16:24.753937 140051752298304 model_training_utils.py:391] Train Step: 2580/5540  / loss = 1.7430394887924194
I0225 05:22:16.407879 140051752298304 model_training_utils.py:391] Train Step: 2590/5540  / loss = 1.2164618968963623
I0225 05:28:06.207026 140051752298304 model_training_utils.py:391] Train Step: 2600/5540  / loss = 1.0873363018035889
I0225 05:33:58.944833 140051752298304 model_training_utils.py:391] Train Step: 2610/5540  / loss = 1.025174856185913
I0225 05:39:50.384715 140051752298304 model_training_utils.py:391] Train Step: 2620/5540  / loss = 1.1241925954818726
I0225 05:45:41.759386 140051752298304 model_training_utils.py:391] Train Step: 2630/5540  / loss = 0.9128037691116333
I0225 05:51:32.703482 140051752298304 model_training_utils.py:391] Train Step: 2640/5540  / loss = 0.9679468870162964
I0225 05:57:24.584220 140051752298304 model_training_utils.py:391] Train Step: 2650/5540  / loss = 1.1830971240997314
I0225 06:03:17.533166 140051752298304 model_training_utils.py:391] Train Step: 2660/5540  / loss = 1.1761590242385864
I0225 06:09:10.260724 140051752298304 model_training_utils.py:391] Train Step: 2670/5540  / loss = 0.8787859678268433
I0225 06:15:05.515945 140051752298304 model_training_utils.py:391] Train Step: 2680/5540  / loss = 0.7102453708648682
I0225 06:20:58.225967 140051752298304 model_training_utils.py:391] Train Step: 2690/5540  / loss = 0.6507004499435425
I0225 06:26:50.017932 140051752298304 model_training_utils.py:391] Train Step: 2700/5540  / loss = 1.076472520828247
I0225 06:32:42.075260 140051752298304 model_training_utils.py:391] Train Step: 2710/5540  / loss = 0.7566004395484924
I0225 06:38:37.855805 140051752298304 model_training_utils.py:391] Train Step: 2720/5540  / loss = 0.6017062664031982
I0225 06:44:28.096819 140051752298304 model_training_utils.py:391] Train Step: 2730/5540  / loss = 1.0214755535125732
I0225 06:50:21.403689 140051752298304 model_training_utils.py:391] Train Step: 2740/5540  / loss = 1.1210113763809204
I0225 06:56:13.551277 140051752298304 model_training_utils.py:391] Train Step: 2750/5540  / loss = 0.8481057286262512
I0225 07:02:05.931581 140051752298304 model_training_utils.py:391] Train Step: 2760/5540  / loss = 0.8500322103500366
I0225 07:07:59.460455 140051752298304 model_training_utils.py:391] Train Step: 2770/5540  / loss = 0.6402630805969238
I0225 07:08:02.338669 140051752298304 model_training_utils.py:38] Saving model as TF checkpoint: /nfs/site/home/jojimonv/Models/TF2.0/BERT/models/squad_model_dir/ctl_step_2770.ckpt-1
I0225 07:13:52.862276 140051752298304 model_training_utils.py:391] Train Step: 2780/5540  / loss = 0.6911314725875854
I0225 07:19:44.181512 140051752298304 model_training_utils.py:391] Train Step: 2790/5540  / loss = 1.0374987125396729
I0225 07:25:34.818951 140051752298304 model_training_utils.py:391] Train Step: 2800/5540  / loss = 0.9891743659973145
I0225 07:31:28.664759 140051752298304 model_training_utils.py:391] Train Step: 2810/5540  / loss = 0.891287624835968
I0225 07:37:17.381081 140051752298304 model_training_utils.py:391] Train Step: 2820/5540  / loss = 1.055591344833374
I0225 07:43:09.105816 140051752298304 model_training_utils.py:391] Train Step: 2830/5540  / loss = 0.8469554781913757
I0225 07:49:00.001422 140051752298304 model_training_utils.py:391] Train Step: 2840/5540  / loss = 0.8605449795722961
I0225 07:54:51.576237 140051752298304 model_training_utils.py:391] Train Step: 2850/5540  / loss = 0.9651020169258118
I0225 08:00:46.262902 140051752298304 model_training_utils.py:391] Train Step: 2860/5540  / loss = 0.893596351146698
I0225 08:06:38.388895 140051752298304 model_training_utils.py:391] Train Step: 2870/5540  / loss = 0.9058836102485657
I0225 08:12:29.808654 140051752298304 model_training_utils.py:391] Train Step: 2880/5540  / loss = 1.0444872379302979
I0225 08:18:20.569463 140051752298304 model_training_utils.py:391] Train Step: 2890/5540  / loss = 0.8372774124145508
I0225 08:24:12.497051 140051752298304 model_training_utils.py:391] Train Step: 2900/5540  / loss = 0.604779064655304
I0225 08:30:02.666839 140051752298304 model_training_utils.py:391] Train Step: 2910/5540  / loss = 0.5718136429786682
I0225 08:35:54.666485 140051752298304 model_training_utils.py:391] Train Step: 2920/5540  / loss = 0.6456276178359985
I0225 08:41:44.033912 140051752298304 model_training_utils.py:391] Train Step: 2930/5540  / loss = 0.9701265096664429
I0225 08:47:34.577281 140051752298304 model_training_utils.py:391] Train Step: 2940/5540  / loss = 1.0655694007873535
I0225 08:53:23.655282 140051752298304 model_training_utils.py:391] Train Step: 2950/5540  / loss = 0.8798655271530151
I0225 08:59:10.948026 140051752298304 model_training_utils.py:391] Train Step: 2960/5540  / loss = 0.8278633952140808
I0225 09:05:00.034434 140051752298304 model_training_utils.py:391] Train Step: 2970/5540  / loss = 0.8840143084526062
I0225 09:10:50.761779 140051752298304 model_training_utils.py:391] Train Step: 2980/5540  / loss = 1.1128658056259155
I0225 09:16:42.216319 140051752298304 model_training_utils.py:391] Train Step: 2990/5540  / loss = 0.8282062411308289
I0225 09:22:31.305410 140051752298304 model_training_utils.py:391] Train Step: 3000/5540  / loss = 0.7446919679641724
I0225 09:28:21.444503 140051752298304 model_training_utils.py:391] Train Step: 3010/5540  / loss = 0.4696410596370697
I0225 09:34:10.332988 140051752298304 model_training_utils.py:391] Train Step: 3020/5540  / loss = 0.7839628458023071
I0225 09:40:00.101686 140051752298304 model_training_utils.py:391] Train Step: 3030/5540  / loss = 0.7895044684410095
I0225 09:45:48.580670 140051752298304 model_training_utils.py:391] Train Step: 3040/5540  / loss = 0.843948245048523
I0225 09:51:39.141750 140051752298304 model_training_utils.py:391] Train Step: 3050/5540  / loss = 1.237268090248108
I0225 09:57:29.115553 140051752298304 model_training_utils.py:391] Train Step: 3060/5540  / loss = 0.9587383270263672
I0225 10:03:17.809768 140051752298304 model_training_utils.py:391] Train Step: 3070/5540  / loss = 0.7109512686729431
I0225 10:09:05.118091 140051752298304 model_training_utils.py:391] Train Step: 3080/5540  / loss = 0.46810227632522583
I0225 10:14:56.822450 140051752298304 model_training_utils.py:391] Train Step: 3090/5540  / loss = 0.3955478370189667
I0225 10:20:45.853753 140051752298304 model_training_utils.py:391] Train Step: 3100/5540  / loss = 1.0818804502487183
I0225 10:26:34.601817 140051752298304 model_training_utils.py:391] Train Step: 3110/5540  / loss = 0.8671382069587708
I0225 10:32:25.262773 140051752298304 model_training_utils.py:391] Train Step: 3120/5540  / loss = 0.7238496541976929
I0225 10:38:16.283201 140051752298304 model_training_utils.py:391] Train Step: 3130/5540  / loss = 0.49795204401016235
I0225 10:44:06.337998 140051752298304 model_training_utils.py:391] Train Step: 3140/5540  / loss = 0.4259694218635559
I0225 10:49:56.466891 140051752298304 model_training_utils.py:391] Train Step: 3150/5540  / loss = 0.48269376158714294
I0225 10:55:45.848641 140051752298304 model_training_utils.py:391] Train Step: 3160/5540  / loss = 0.5092756152153015
I0225 11:01:36.614446 140051752298304 model_training_utils.py:391] Train Step: 3170/5540  / loss = 0.488680899143219
I0225 11:07:25.794045 140051752298304 model_training_utils.py:391] Train Step: 3180/5540  / loss = 0.5635273456573486
I0225 11:13:16.926830 140051752298304 model_training_utils.py:391] Train Step: 3190/5540  / loss = 0.6251615285873413
I0225 11:19:06.163133 140051752298304 model_training_utils.py:391] Train Step: 3200/5540  / loss = 0.3942922055721283
I0225 11:24:55.286960 140051752298304 model_training_utils.py:391] Train Step: 3210/5540  / loss = 0.6951661109924316
I0225 11:30:45.874255 140051752298304 model_training_utils.py:391] Train Step: 3220/5540  / loss = 0.4836098253726959
I0225 11:36:33.336797 140051752298304 model_training_utils.py:391] Train Step: 3230/5540  / loss = 0.638471782207489
I0225 11:42:23.482174 140051752298304 model_training_utils.py:391] Train Step: 3240/5540  / loss = 0.5799443125724792
I0225 11:48:12.019167 140051752298304 model_training_utils.py:391] Train Step: 3250/5540  / loss = 0.6080964803695679
I0225 11:54:01.952330 140051752298304 model_training_utils.py:391] Train Step: 3260/5540  / loss = 0.5274062156677246
I0225 11:59:50.457027 140051752298304 model_training_utils.py:391] Train Step: 3270/5540  / loss = 0.677374005317688
I0225 12:05:40.910048 140051752298304 model_training_utils.py:391] Train Step: 3280/5540  / loss = 0.7130151987075806
I0225 12:11:28.945737 140051752298304 model_training_utils.py:391] Train Step: 3290/5540  / loss = 0.40108710527420044
I0225 12:17:19.251525 140051752298304 model_training_utils.py:391] Train Step: 3300/5540  / loss = 0.48107433319091797
I0225 12:23:09.011292 140051752298304 model_training_utils.py:391] Train Step: 3310/5540  / loss = 0.6402803063392639
I0225 12:28:59.328613 140051752298304 model_training_utils.py:391] Train Step: 3320/5540  / loss = 0.6603633761405945
I0225 12:34:48.129895 140051752298304 model_training_utils.py:391] Train Step: 3330/5540  / loss = 0.53838050365448
I0225 12:40:35.772473 140051752298304 model_training_utils.py:391] Train Step: 3340/5540  / loss = 0.4809506833553314
I0225 12:46:25.013226 140051752298304 model_training_utils.py:391] Train Step: 3350/5540  / loss = 0.5882676839828491
I0225 12:52:13.717830 140051752298304 model_training_utils.py:391] Train Step: 3360/5540  / loss = 0.432852178812027
I0225 12:58:01.970906 140051752298304 model_training_utils.py:391] Train Step: 3370/5540  / loss = 0.3743130564689636
I0225 13:03:51.588486 140051752298304 model_training_utils.py:391] Train Step: 3380/5540  / loss = 0.2993309199810028
I0225 13:09:42.400395 140051752298304 model_training_utils.py:391] Train Step: 3390/5540  / loss = 0.6171059608459473
I0225 13:15:31.495082 140051752298304 model_training_utils.py:391] Train Step: 3400/5540  / loss = 0.5770093202590942
I0225 13:21:20.579690 140051752298304 model_training_utils.py:391] Train Step: 3410/5540  / loss = 0.6269583702087402
I0225 13:27:09.072372 140051752298304 model_training_utils.py:391] Train Step: 3420/5540  / loss = 0.8258465528488159
I0225 13:32:58.347347 140051752298304 model_training_utils.py:391] Train Step: 3430/5540  / loss = 0.47834110260009766
I0225 13:38:49.832379 140051752298304 model_training_utils.py:391] Train Step: 3440/5540  / loss = 0.5799750089645386
I0225 13:44:39.267254 140051752298304 model_training_utils.py:391] Train Step: 3450/5540  / loss = 0.3666127920150757
I0225 13:50:27.018198 140051752298304 model_training_utils.py:391] Train Step: 3460/5540  / loss = 0.5937392711639404
I0225 13:56:16.395247 140051752298304 model_training_utils.py:391] Train Step: 3470/5540  / loss = 0.578057050704956
I0225 14:02:05.461115 140051752298304 model_training_utils.py:391] Train Step: 3480/5540  / loss = 0.39291101694107056
I0225 14:07:56.143297 140051752298304 model_training_utils.py:391] Train Step: 3490/5540  / loss = 0.4610482156276703
I0225 14:13:48.746679 140051752298304 model_training_utils.py:391] Train Step: 3500/5540  / loss = 0.3579186201095581
I0225 14:19:38.560213 140051752298304 model_training_utils.py:391] Train Step: 3510/5540  / loss = 0.34156832098960876
I0225 14:25:27.486079 140051752298304 model_training_utils.py:391] Train Step: 3520/5540  / loss = 0.31736499071121216
I0225 14:31:17.793104 140051752298304 model_training_utils.py:391] Train Step: 3530/5540  / loss = 0.9367548227310181
I0225 14:37:06.487930 140051752298304 model_training_utils.py:391] Train Step: 3540/5540  / loss = 0.784968912601471
I0225 14:42:55.164816 140051752298304 model_training_utils.py:391] Train Step: 3550/5540  / loss = 0.45641160011291504
I0225 14:48:44.404623 140051752298304 model_training_utils.py:391] Train Step: 3560/5540  / loss = 0.5972925424575806
I0225 14:54:33.741647 140051752298304 model_training_utils.py:391] Train Step: 3570/5540  / loss = 0.5291450619697571
I0225 15:00:22.157358 140051752298304 model_training_utils.py:391] Train Step: 3580/5540  / loss = 0.5051387548446655
I0225 15:06:12.640130 140051752298304 model_training_utils.py:391] Train Step: 3590/5540  / loss = 0.5624746084213257
I0225 15:12:03.440172 140051752298304 model_training_utils.py:391] Train Step: 3600/5540  / loss = 0.39233413338661194
I0225 15:17:53.121809 140051752298304 model_training_utils.py:391] Train Step: 3610/5540  / loss = 0.44454604387283325
I0225 15:23:45.528887 140051752298304 model_training_utils.py:391] Train Step: 3620/5540  / loss = 0.5347380638122559
I0225 15:29:36.228307 140051752298304 model_training_utils.py:391] Train Step: 3630/5540  / loss = 0.584750771522522
I0225 15:35:27.158594 140051752298304 model_training_utils.py:391] Train Step: 3640/5540  / loss = 0.5955935716629028
I0225 15:41:19.935292 140051752298304 model_training_utils.py:391] Train Step: 3650/5540  / loss = 0.49527668952941895
I0225 15:47:09.307251 140051752298304 model_training_utils.py:391] Train Step: 3660/5540  / loss = 0.5508665442466736
I0225 15:52:59.107869 140051752298304 model_training_utils.py:391] Train Step: 3670/5540  / loss = 0.3701898455619812
I0225 15:58:49.981223 140051752298304 model_training_utils.py:391] Train Step: 3680/5540  / loss = 0.44687971472740173
I0225 16:04:38.308604 140051752298304 model_training_utils.py:391] Train Step: 3690/5540  / loss = 0.48924294114112854
I0225 16:10:27.075187 140051752298304 model_training_utils.py:391] Train Step: 3700/5540  / loss = 0.7473818063735962
I0225 16:16:18.694110 140051752298304 model_training_utils.py:391] Train Step: 3710/5540  / loss = 0.5664001107215881
I0225 16:22:08.604873 140051752298304 model_training_utils.py:391] Train Step: 3720/5540  / loss = 0.439251184463501
I0225 16:27:58.996301 140051752298304 model_training_utils.py:391] Train Step: 3730/5540  / loss = 0.8569095730781555
I0225 16:33:49.373369 140051752298304 model_training_utils.py:391] Train Step: 3740/5540  / loss = 0.8476439714431763
I0225 16:39:40.330979 140051752298304 model_training_utils.py:391] Train Step: 3750/5540  / loss = 0.6685463190078735
I0225 16:45:29.318242 140051752298304 model_training_utils.py:391] Train Step: 3760/5540  / loss = 0.46781253814697266
I0225 16:51:20.256589 140051752298304 model_training_utils.py:391] Train Step: 3770/5540  / loss = 0.3518163561820984
I0225 16:57:11.302518 140051752298304 model_training_utils.py:391] Train Step: 3780/5540  / loss = 0.3880344033241272
I0225 17:03:01.101501 140051752298304 model_training_utils.py:391] Train Step: 3790/5540  / loss = 0.48430976271629333
I0225 17:08:51.124362 140051752298304 model_training_utils.py:391] Train Step: 3800/5540  / loss = 0.4679547846317291
I0225 17:14:38.641178 140051752298304 model_training_utils.py:391] Train Step: 3810/5540  / loss = 0.37326449155807495
I0225 17:20:26.855454 140051752298304 model_training_utils.py:391] Train Step: 3820/5540  / loss = 0.43683019280433655
I0225 17:26:15.800782 140051752298304 model_training_utils.py:391] Train Step: 3830/5540  / loss = 0.4747883379459381
I0225 17:32:03.896673 140051752298304 model_training_utils.py:391] Train Step: 3840/5540  / loss = 0.4757099151611328
I0225 17:37:54.601034 140051752298304 model_training_utils.py:391] Train Step: 3850/5540  / loss = 0.5517746210098267
I0225 17:43:42.167336 140051752298304 model_training_utils.py:391] Train Step: 3860/5540  / loss = 0.7438861131668091
I0225 17:49:31.649377 140051752298304 model_training_utils.py:391] Train Step: 3870/5540  / loss = 1.3362250328063965
I0225 17:55:22.309373 140051752298304 model_training_utils.py:391] Train Step: 3880/5540  / loss = 0.452596515417099
I0225 18:01:11.290210 140051752298304 model_training_utils.py:391] Train Step: 3890/5540  / loss = 0.581070601940155
I0225 18:07:02.523603 140051752298304 model_training_utils.py:391] Train Step: 3900/5540  / loss = 1.2032840251922607
I0225 18:12:52.158542 140051752298304 model_training_utils.py:391] Train Step: 3910/5540  / loss = 0.884655773639679
I0225 18:18:41.973189 140051752298304 model_training_utils.py:391] Train Step: 3920/5540  / loss = 1.1840331554412842
I0225 18:24:31.018043 140051752298304 model_training_utils.py:391] Train Step: 3930/5540  / loss = 0.5073528289794922
I0225 18:30:20.283364 140051752298304 model_training_utils.py:391] Train Step: 3940/5540  / loss = 0.4439801573753357
I0225 18:36:11.201919 140051752298304 model_training_utils.py:391] Train Step: 3950/5540  / loss = 0.4931264817714691
I0225 18:42:02.293558 140051752298304 model_training_utils.py:391] Train Step: 3960/5540  / loss = 0.638611376285553
I0225 18:47:50.503352 140051752298304 model_training_utils.py:391] Train Step: 3970/5540  / loss = 0.5546340346336365
I0225 18:53:42.380733 140051752298304 model_training_utils.py:391] Train Step: 3980/5540  / loss = 0.4209218919277191
I0225 18:59:30.352129 140051752298304 model_training_utils.py:391] Train Step: 3990/5540  / loss = 0.40405216813087463
I0225 19:05:19.090088 140051752298304 model_training_utils.py:391] Train Step: 4000/5540  / loss = 0.3669317364692688
I0225 19:11:09.615630 140051752298304 model_training_utils.py:391] Train Step: 4010/5540  / loss = 0.3607334494590759
I0225 19:16:59.471829 140051752298304 model_training_utils.py:391] Train Step: 4020/5540  / loss = 1.1777091026306152
I0225 19:22:48.474873 140051752298304 model_training_utils.py:391] Train Step: 4030/5540  / loss = 0.4062556326389313
I0225 19:28:38.770612 140051752298304 model_training_utils.py:391] Train Step: 4040/5540  / loss = 1.1843453645706177
I0225 19:34:30.782680 140051752298304 model_training_utils.py:391] Train Step: 4050/5540  / loss = 0.6220107674598694
I0225 19:40:20.476677 140051752298304 model_training_utils.py:391] Train Step: 4060/5540  / loss = 0.6044301390647888
I0225 19:46:10.210860 140051752298304 model_training_utils.py:391] Train Step: 4070/5540  / loss = 0.5310906171798706
I0225 19:51:59.850763 140051752298304 model_training_utils.py:391] Train Step: 4080/5540  / loss = 0.5942870378494263
I0225 19:57:49.992165 140051752298304 model_training_utils.py:391] Train Step: 4090/5540  / loss = 0.7925225496292114
I0225 20:03:39.393384 140051752298304 model_training_utils.py:391] Train Step: 4100/5540  / loss = 0.6087749004364014
I0225 20:09:31.079419 140051752298304 model_training_utils.py:391] Train Step: 4110/5540  / loss = 0.40487924218177795
I0225 20:15:20.892677 140051752298304 model_training_utils.py:391] Train Step: 4120/5540  / loss = 0.5745726227760315
I0225 20:21:11.175567 140051752298304 model_training_utils.py:391] Train Step: 4130/5540  / loss = 1.1151001453399658
I0225 20:27:02.369126 140051752298304 model_training_utils.py:391] Train Step: 4140/5540  / loss = 0.7911396026611328
I0225 20:32:55.057729 140051752298304 model_training_utils.py:391] Train Step: 4150/5540  / loss = 0.6002322435379028
I0225 20:38:45.985801 140051752298304 model_training_utils.py:391] Train Step: 4160/5540  / loss = 0.5929166078567505
I0225 20:44:36.307582 140051752298304 model_training_utils.py:391] Train Step: 4170/5540  / loss = 0.6004466414451599
I0225 20:50:29.367458 140051752298304 model_training_utils.py:391] Train Step: 4180/5540  / loss = 0.4900449216365814
I0225 20:56:20.186441 140051752298304 model_training_utils.py:391] Train Step: 4190/5540  / loss = 0.6977471709251404
I0225 21:02:10.905735 140051752298304 model_training_utils.py:391] Train Step: 4200/5540  / loss = 1.1396281719207764
I0225 21:08:02.548975 140051752298304 model_training_utils.py:391] Train Step: 4210/5540  / loss = 0.7220929861068726
I0225 21:13:52.751045 140051752298304 model_training_utils.py:391] Train Step: 4220/5540  / loss = 0.8931759595870972
I0225 21:19:44.979253 140051752298304 model_training_utils.py:391] Train Step: 4230/5540  / loss = 0.6552034616470337
I0225 21:25:36.390128 140051752298304 model_training_utils.py:391] Train Step: 4240/5540  / loss = 0.9382546544075012
I0225 21:31:27.461496 140051752298304 model_training_utils.py:391] Train Step: 4250/5540  / loss = 0.46568790078163147
I0225 21:37:18.867800 140051752298304 model_training_utils.py:391] Train Step: 4260/5540  / loss = 0.5442288517951965
I0225 21:43:09.484621 140051752298304 model_training_utils.py:391] Train Step: 4270/5540  / loss = 1.3668949604034424
I0225 21:49:00.003467 140051752298304 model_training_utils.py:391] Train Step: 4280/5540  / loss = 1.4363614320755005
I0225 21:54:50.845767 140051752298304 model_training_utils.py:391] Train Step: 4290/5540  / loss = 0.719366729259491
I0225 22:00:41.370525 140051752298304 model_training_utils.py:391] Train Step: 4300/5540  / loss = 0.5763965845108032
I0225 22:06:30.706839 140051752298304 model_training_utils.py:391] Train Step: 4310/5540  / loss = 1.3764255046844482
I0225 22:12:20.658967 140051752298304 model_training_utils.py:391] Train Step: 4320/5540  / loss = 0.9874507188796997
I0225 22:18:09.978173 140051752298304 model_training_utils.py:391] Train Step: 4330/5540  / loss = 0.44872719049453735
I0225 22:24:02.652459 140051752298304 model_training_utils.py:391] Train Step: 4340/5540  / loss = 0.6810978055000305
I0225 22:29:54.694353 140051752298304 model_training_utils.py:391] Train Step: 4350/5540  / loss = 0.4322306513786316
I0225 22:35:44.266381 140051752298304 model_training_utils.py:391] Train Step: 4360/5540  / loss = 0.3015810549259186
I0225 22:41:34.877362 140051752298304 model_training_utils.py:391] Train Step: 4370/5540  / loss = 0.45457911491394043
I0225 22:47:26.054926 140051752298304 model_training_utils.py:391] Train Step: 4380/5540  / loss = 0.6985858678817749
I0225 22:53:17.207798 140051752298304 model_training_utils.py:391] Train Step: 4390/5540  / loss = 0.5654333829879761
I0225 22:59:06.768405 140051752298304 model_training_utils.py:391] Train Step: 4400/5540  / loss = 0.8592921495437622
I0225 23:04:57.216084 140051752298304 model_training_utils.py:391] Train Step: 4410/5540  / loss = 0.4682844579219818
I0225 23:10:46.017508 140051752298304 model_training_utils.py:391] Train Step: 4420/5540  / loss = 0.3956855535507202
I0225 23:16:35.818013 140051752298304 model_training_utils.py:391] Train Step: 4430/5540  / loss = 0.6530824899673462
I0225 23:22:25.525078 140051752298304 model_training_utils.py:391] Train Step: 4440/5540  / loss = 0.7752544283866882
I0225 23:28:15.500174 140051752298304 model_training_utils.py:391] Train Step: 4450/5540  / loss = 0.8508715629577637
I0225 23:34:05.161362 140051752298304 model_training_utils.py:391] Train Step: 4460/5540  / loss = 1.083814024925232
I0225 23:39:55.953099 140051752298304 model_training_utils.py:391] Train Step: 4470/5540  / loss = 0.9011780619621277
I0225 23:45:45.351054 140051752298304 model_training_utils.py:391] Train Step: 4480/5540  / loss = 0.537807822227478
I0225 23:51:35.274500 140051752298304 model_training_utils.py:391] Train Step: 4490/5540  / loss = 0.607476532459259
I0225 23:57:26.319586 140051752298304 model_training_utils.py:391] Train Step: 4500/5540  / loss = 0.5375041365623474
I0226 00:03:17.403240 140051752298304 model_training_utils.py:391] Train Step: 4510/5540  / loss = 0.3656221032142639
I0226 00:09:06.486300 140051752298304 model_training_utils.py:391] Train Step: 4520/5540  / loss = 1.1722705364227295
I0226 00:14:57.465227 140051752298304 model_training_utils.py:391] Train Step: 4530/5540  / loss = 0.7090349197387695
I0226 00:20:46.882829 140051752298304 model_training_utils.py:391] Train Step: 4540/5540  / loss = 0.4291091859340668
I0226 00:26:35.617804 140051752298304 model_training_utils.py:391] Train Step: 4550/5540  / loss = 0.765281617641449
I0226 00:32:23.893340 140051752298304 model_training_utils.py:391] Train Step: 4560/5540  / loss = 0.6194828748703003
I0226 00:38:14.187862 140051752298304 model_training_utils.py:391] Train Step: 4570/5540  / loss = 0.6215421557426453
I0226 00:44:03.530700 140051752298304 model_training_utils.py:391] Train Step: 4580/5540  / loss = 0.9377347230911255
I0226 00:49:52.283190 140051752298304 model_training_utils.py:391] Train Step: 4590/5540  / loss = 1.1429526805877686
I0226 00:55:43.209958 140051752298304 model_training_utils.py:391] Train Step: 4600/5540  / loss = 0.6647316217422485
I0226 01:01:33.299377 140051752298304 model_training_utils.py:391] Train Step: 4610/5540  / loss = 0.5165330171585083
I0226 01:07:22.995751 140051752298304 model_training_utils.py:391] Train Step: 4620/5540  / loss = 0.5470191240310669
I0226 01:13:14.545529 140051752298304 model_training_utils.py:391] Train Step: 4630/5540  / loss = 0.830970287322998
I0226 01:19:02.179495 140051752298304 model_training_utils.py:391] Train Step: 4640/5540  / loss = 0.47876524925231934
I0226 01:24:51.017716 140051752298304 model_training_utils.py:391] Train Step: 4650/5540  / loss = 0.5892942547798157
I0226 01:30:41.086391 140051752298304 model_training_utils.py:391] Train Step: 4660/5540  / loss = 0.34250393509864807
I0226 01:36:31.525049 140051752298304 model_training_utils.py:391] Train Step: 4670/5540  / loss = 0.6710494160652161
I0226 01:42:21.663271 140051752298304 model_training_utils.py:391] Train Step: 4680/5540  / loss = 0.37632569670677185
I0226 01:48:10.885563 140051752298304 model_training_utils.py:391] Train Step: 4690/5540  / loss = 0.31960609555244446
I0226 01:54:01.539912 140051752298304 model_training_utils.py:391] Train Step: 4700/5540  / loss = 0.4554426074028015
I0226 01:59:51.074975 140051752298304 model_training_utils.py:391] Train Step: 4710/5540  / loss = 0.44655507802963257
I0226 02:05:40.360277 140051752298304 model_training_utils.py:391] Train Step: 4720/5540  / loss = 0.43878239393234253
I0226 02:11:29.881732 140051752298304 model_training_utils.py:391] Train Step: 4730/5540  / loss = 0.6553900837898254
I0226 02:17:19.860735 140051752298304 model_training_utils.py:391] Train Step: 4740/5540  / loss = 0.4168780744075775
I0226 02:23:08.424155 140051752298304 model_training_utils.py:391] Train Step: 4750/5540  / loss = 0.5802512764930725
I0226 02:28:56.497135 140051752298304 model_training_utils.py:391] Train Step: 4760/5540  / loss = 0.7807776927947998
I0226 02:34:46.012799 140051752298304 model_training_utils.py:391] Train Step: 4770/5540  / loss = 0.6028302311897278
I0226 02:40:37.830504 140051752298304 model_training_utils.py:391] Train Step: 4780/5540  / loss = 0.5898401737213135
I0226 02:46:28.131530 140051752298304 model_training_utils.py:391] Train Step: 4790/5540  / loss = 0.38405272364616394
I0226 02:52:16.851048 140051752298304 model_training_utils.py:391] Train Step: 4800/5540  / loss = 0.46948176622390747
I0226 02:58:08.171295 140051752298304 model_training_utils.py:391] Train Step: 4810/5540  / loss = 0.5786730051040649
I0226 03:03:57.778918 140051752298304 model_training_utils.py:391] Train Step: 4820/5540  / loss = 0.5084900856018066
I0226 03:09:45.496461 140051752298304 model_training_utils.py:391] Train Step: 4830/5540  / loss = 0.3911944329738617
I0226 03:15:35.564624 140051752298304 model_training_utils.py:391] Train Step: 4840/5540  / loss = 0.4001947343349457
I0226 03:21:23.772593 140051752298304 model_training_utils.py:391] Train Step: 4850/5540  / loss = 0.39769017696380615
I0226 03:27:13.000528 140051752298304 model_training_utils.py:391] Train Step: 4860/5540  / loss = 0.6715112328529358
I0226 03:33:01.867869 140051752298304 model_training_utils.py:391] Train Step: 4870/5540  / loss = 0.5398823618888855
I0226 03:38:51.450467 140051752298304 model_training_utils.py:391] Train Step: 4880/5540  / loss = 0.7198427319526672
I0226 03:44:41.021383 140051752298304 model_training_utils.py:391] Train Step: 4890/5540  / loss = 0.6003713011741638
I0226 03:50:30.900556 140051752298304 model_training_utils.py:391] Train Step: 4900/5540  / loss = 0.5387272834777832
I0226 03:56:20.812154 140051752298304 model_training_utils.py:391] Train Step: 4910/5540  / loss = 0.36176204681396484
I0226 04:02:09.004397 140051752298304 model_training_utils.py:391] Train Step: 4920/5540  / loss = 0.5517615675926208
I0226 04:08:00.987949 140051752298304 model_training_utils.py:391] Train Step: 4930/5540  / loss = 0.6103891730308533
I0226 04:13:52.428460 140051752298304 model_training_utils.py:391] Train Step: 4940/5540  / loss = 0.6416193246841431
I0226 04:19:44.077326 140051752298304 model_training_utils.py:391] Train Step: 4950/5540  / loss = 0.4445926249027252
I0226 04:25:33.915834 140051752298304 model_training_utils.py:391] Train Step: 4960/5540  / loss = 0.5575617551803589
I0226 04:31:24.469707 140051752298304 model_training_utils.py:391] Train Step: 4970/5540  / loss = 0.5148875713348389
I0226 04:37:14.514346 140051752298304 model_training_utils.py:391] Train Step: 4980/5540  / loss = 0.43197017908096313
I0226 04:43:04.568740 140051752298304 model_training_utils.py:391] Train Step: 4990/5540  / loss = 0.8794981837272644
I0226 04:48:54.740664 140051752298304 model_training_utils.py:391] Train Step: 5000/5540  / loss = 1.7040218114852905
I0226 04:54:46.709010 140051752298304 model_training_utils.py:391] Train Step: 5010/5540  / loss = 1.0606379508972168
I0226 05:00:36.767110 140051752298304 model_training_utils.py:391] Train Step: 5020/5540  / loss = 0.3844143748283386
I0226 05:06:26.302215 140051752298304 model_training_utils.py:391] Train Step: 5030/5540  / loss = 0.3337458670139313
I0226 05:12:15.204233 140051752298304 model_training_utils.py:391] Train Step: 5040/5540  / loss = 0.4833891987800598
I0226 05:18:04.992929 140051752298304 model_training_utils.py:391] Train Step: 5050/5540  / loss = 0.650698184967041
I0226 05:23:55.113611 140051752298304 model_training_utils.py:391] Train Step: 5060/5540  / loss = 1.2307422161102295
I0226 05:29:44.801885 140051752298304 model_training_utils.py:391] Train Step: 5070/5540  / loss = 0.47122830152511597
I0226 05:35:34.806406 140051752298304 model_training_utils.py:391] Train Step: 5080/5540  / loss = 0.5023813843727112
I0226 05:41:25.135516 140051752298304 model_training_utils.py:391] Train Step: 5090/5540  / loss = 0.8143801689147949
I0226 05:47:14.951363 140051752298304 model_training_utils.py:391] Train Step: 5100/5540  / loss = 0.5123795866966248
I0226 05:53:04.375945 140051752298304 model_training_utils.py:391] Train Step: 5110/5540  / loss = 1.6011234521865845
I0226 05:58:53.845119 140051752298304 model_training_utils.py:391] Train Step: 5120/5540  / loss = 0.7385988831520081
I0226 06:04:42.374204 140051752298304 model_training_utils.py:391] Train Step: 5130/5540  / loss = 0.5527450442314148
I0226 06:10:30.725138 140051752298304 model_training_utils.py:391] Train Step: 5140/5540  / loss = 0.8247378468513489
I0226 06:16:22.015706 140051752298304 model_training_utils.py:391] Train Step: 5150/5540  / loss = 0.8121041059494019
I0226 06:22:10.730122 140051752298304 model_training_utils.py:391] Train Step: 5160/5540  / loss = 0.901097297668457
I0226 06:27:59.211860 140051752298304 model_training_utils.py:391] Train Step: 5170/5540  / loss = 0.5766862630844116
I0226 06:33:48.671473 140051752298304 model_training_utils.py:391] Train Step: 5180/5540  / loss = 0.4307521879673004
I0226 06:39:39.212958 140051752298304 model_training_utils.py:391] Train Step: 5190/5540  / loss = 0.5503541827201843
I0226 06:45:28.420723 140051752298304 model_training_utils.py:391] Train Step: 5200/5540  / loss = 0.40966668725013733
I0226 06:51:18.956278 140051752298304 model_training_utils.py:391] Train Step: 5210/5540  / loss = 0.46479958295822144
I0226 06:57:08.426595 140051752298304 model_training_utils.py:391] Train Step: 5220/5540  / loss = 0.7956053018569946
I0226 07:02:58.498655 140051752298304 model_training_utils.py:391] Train Step: 5230/5540  / loss = 1.6142280101776123
I0226 07:08:48.800669 140051752298304 model_training_utils.py:391] Train Step: 5240/5540  / loss = 0.7637634873390198
I0226 07:14:39.563160 140051752298304 model_training_utils.py:391] Train Step: 5250/5540  / loss = 0.5643676519393921
I0226 07:20:32.015982 140051752298304 model_training_utils.py:391] Train Step: 5260/5540  / loss = 0.36723726987838745
I0226 07:26:22.541232 140051752298304 model_training_utils.py:391] Train Step: 5270/5540  / loss = 0.3037044405937195
I0226 07:32:09.702478 140051752298304 model_training_utils.py:391] Train Step: 5280/5540  / loss = 0.5000852942466736
I0226 07:37:58.465891 140051752298304 model_training_utils.py:391] Train Step: 5290/5540  / loss = 0.6599246263504028
I0226 07:43:48.684066 140051752298304 model_training_utils.py:391] Train Step: 5300/5540  / loss = 0.48584023118019104
I0226 07:49:37.601109 140051752298304 model_training_utils.py:391] Train Step: 5310/5540  / loss = 0.5065387487411499
I0226 07:55:26.228548 140051752298304 model_training_utils.py:391] Train Step: 5320/5540  / loss = 0.425847589969635
I0226 08:01:16.435595 140051752298304 model_training_utils.py:391] Train Step: 5330/5540  / loss = 0.5366352200508118
I0226 08:07:06.504623 140051752298304 model_training_utils.py:391] Train Step: 5340/5540  / loss = 0.5033913850784302
I0226 08:12:55.908948 140051752298304 model_training_utils.py:391] Train Step: 5350/5540  / loss = 1.236319661140442
I0226 08:18:44.077682 140051752298304 model_training_utils.py:391] Train Step: 5360/5540  / loss = 0.6076527833938599
I0226 08:24:32.900590 140051752298304 model_training_utils.py:391] Train Step: 5370/5540  / loss = 0.6166521310806274
I0226 08:30:23.547903 140051752298304 model_training_utils.py:391] Train Step: 5380/5540  / loss = 0.49764150381088257
I0226 08:36:14.828857 140051752298304 model_training_utils.py:391] Train Step: 5390/5540  / loss = 0.5932551622390747
I0226 08:42:06.487151 140051752298304 model_training_utils.py:391] Train Step: 5400/5540  / loss = 0.5024547576904297
I0226 08:47:58.810095 140051752298304 model_training_utils.py:391] Train Step: 5410/5540  / loss = 0.48518234491348267
I0226 08:53:48.948148 140051752298304 model_training_utils.py:391] Train Step: 5420/5540  / loss = 0.5886267423629761
I0226 08:59:40.904585 140051752298304 model_training_utils.py:391] Train Step: 5430/5540  / loss = 0.5735111236572266
I0226 09:05:32.902703 140051752298304 model_training_utils.py:391] Train Step: 5440/5540  / loss = 0.5186351537704468
I0226 09:11:22.541869 140051752298304 model_training_utils.py:391] Train Step: 5450/5540  / loss = 0.3669753968715668
I0226 09:17:12.564988 140051752298304 model_training_utils.py:391] Train Step: 5460/5540  / loss = 0.31006088852882385
I0226 09:23:03.278486 140051752298304 model_training_utils.py:391] Train Step: 5470/5540  / loss = 0.5301732420921326
I0226 09:28:51.416008 140051752298304 model_training_utils.py:391] Train Step: 5480/5540  / loss = 0.37964773178100586
I0226 09:34:41.784193 140051752298304 model_training_utils.py:391] Train Step: 5490/5540  / loss = 0.30937328934669495
I0226 09:40:30.595974 140051752298304 model_training_utils.py:391] Train Step: 5500/5540  / loss = 0.6514430046081543
I0226 09:46:19.896316 140051752298304 model_training_utils.py:391] Train Step: 5510/5540  / loss = 0.5115025043487549
I0226 09:52:09.091327 140051752298304 model_training_utils.py:391] Train Step: 5520/5540  / loss = 0.4658757150173187
I0226 09:57:57.449161 140051752298304 model_training_utils.py:391] Train Step: 5530/5540  / loss = 0.4013996124267578
I0226 10:03:44.940792 140051752298304 model_training_utils.py:391] Train Step: 5540/5540  / loss = 0.33365291357040405
I0226 10:03:47.815677 140051752298304 model_training_utils.py:38] Saving model as TF checkpoint: /nfs/site/home/jojimonv/Models/TF2.0/BERT/models/squad_model_dir/ctl_step_5540.ckpt-2
I0226 10:03:47.816339 140051752298304 model_training_utils.py:76] Training Summary: 
{'total_training_steps': 5540, 'train_loss': 0.33365291357040405}
2020-02-26 10:03:47.822834: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: 
I0226 10:03:48.582340 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.582452 140051752298304 squad_lib.py:354] unique_id: 1000000000
I0226 10:03:48.582492 140051752298304 squad_lib.py:355] example_index: 0
I0226 10:03:48.582518 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.582591 140051752298304 squad_lib.py:358] tokens: [CLS] which nfl team represented the afc at super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.582655 140051752298304 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123
I0226 10:03:48.582711 140051752298304 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True
I0226 10:03:48.582812 140051752298304 squad_lib.py:368] input_ids: 101 2029 5088 2136 3421 1996 10511 2012 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.582887 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.582960 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.585321 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.585381 140051752298304 squad_lib.py:354] unique_id: 1000000001
I0226 10:03:48.585415 140051752298304 squad_lib.py:355] example_index: 1
I0226 10:03:48.585443 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.585501 140051752298304 squad_lib.py:358] tokens: [CLS] which nfl team represented the nfc at super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.585577 140051752298304 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123
I0226 10:03:48.585634 140051752298304 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True
I0226 10:03:48.585713 140051752298304 squad_lib.py:368] input_ids: 101 2029 5088 2136 3421 1996 22309 2012 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.585786 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.585864 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.588034 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.588086 140051752298304 squad_lib.py:354] unique_id: 1000000002
I0226 10:03:48.588120 140051752298304 squad_lib.py:355] example_index: 2
I0226 10:03:48.588148 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.588206 140051752298304 squad_lib.py:358] tokens: [CLS] where did super bowl 50 take place ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.588267 140051752298304 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123
I0226 10:03:48.588323 140051752298304 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True
I0226 10:03:48.588400 140051752298304 squad_lib.py:368] input_ids: 101 2073 2106 3565 4605 2753 2202 2173 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.588471 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.588541 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.590702 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.590756 140051752298304 squad_lib.py:354] unique_id: 1000000003
I0226 10:03:48.590790 140051752298304 squad_lib.py:355] example_index: 3
I0226 10:03:48.590818 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.590878 140051752298304 squad_lib.py:358] tokens: [CLS] which nfl team won super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.590939 140051752298304 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123
I0226 10:03:48.590996 140051752298304 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True
I0226 10:03:48.591075 140051752298304 squad_lib.py:368] input_ids: 101 2029 5088 2136 2180 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.591148 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.591229 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.593388 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.593439 140051752298304 squad_lib.py:354] unique_id: 1000000004
I0226 10:03:48.593472 140051752298304 squad_lib.py:355] example_index: 4
I0226 10:03:48.593501 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.593560 140051752298304 squad_lib.py:358] tokens: [CLS] what color was used to emphasize the 50th anniversary of the super bowl ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.593620 140051752298304 squad_lib.py:361] token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:15 32:16 33:17 34:17 35:17 36:18 37:19 38:20 39:21 40:21 41:22 42:23 43:24 44:25 45:26 46:26 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:33 55:34 56:35 57:35 58:35 59:36 60:37 61:38 62:39 63:39 64:39 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:53 81:54 82:54 83:55 84:56 85:56 86:56 87:57 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:66 97:66 98:67 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:80 116:81 117:82 118:83 119:83 120:83 121:84 122:84 123:85 124:86 125:87 126:88 127:89 128:89 129:90 130:91 131:92 132:93 133:94 134:95 135:96 136:97 137:98 138:99 139:100 140:100 141:100 142:101 143:101 144:102 145:103 146:104 147:105 148:106 149:107 150:108 151:109 152:110 153:110 154:111 155:112 156:112 157:112 158:112 159:113 160:114 161:115 162:116 163:117 164:118 165:119 166:120 167:121 168:122 169:122 170:122 171:123 172:123
I0226 10:03:48.593676 140051752298304 squad_lib.py:366] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True
I0226 10:03:48.593753 140051752298304 squad_lib.py:368] input_ids: 101 2054 3609 2001 2109 2000 17902 1996 12951 5315 1997 1996 3565 4605 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.593830 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.593919 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.596057 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.596107 140051752298304 squad_lib.py:354] unique_id: 1000000005
I0226 10:03:48.596140 140051752298304 squad_lib.py:355] example_index: 5
I0226 10:03:48.596168 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.596225 140051752298304 squad_lib.py:358] tokens: [CLS] what was the theme of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.596283 140051752298304 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123
I0226 10:03:48.596339 140051752298304 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True
I0226 10:03:48.596415 140051752298304 squad_lib.py:368] input_ids: 101 2054 2001 1996 4323 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.596488 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.596559 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.598726 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.598780 140051752298304 squad_lib.py:354] unique_id: 1000000006
I0226 10:03:48.598815 140051752298304 squad_lib.py:355] example_index: 6
I0226 10:03:48.598843 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.598902 140051752298304 squad_lib.py:358] tokens: [CLS] what day was the game played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.598962 140051752298304 squad_lib.py:361] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123
I0226 10:03:48.599019 140051752298304 squad_lib.py:366] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True
I0226 10:03:48.599098 140051752298304 squad_lib.py:368] input_ids: 101 2054 2154 2001 1996 2208 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.599181 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.599252 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.601363 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.601415 140051752298304 squad_lib.py:354] unique_id: 1000000007
I0226 10:03:48.601449 140051752298304 squad_lib.py:355] example_index: 7
I0226 10:03:48.601477 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.601534 140051752298304 squad_lib.py:358] tokens: [CLS] what is the afc short for ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.601595 140051752298304 squad_lib.py:361] token_to_orig_map: 9:0 10:1 11:2 12:3 13:4 14:5 15:6 16:7 17:8 18:9 19:10 20:11 21:12 22:13 23:14 24:15 25:16 26:17 27:17 28:17 29:18 30:19 31:20 32:21 33:21 34:22 35:23 36:24 37:25 38:26 39:26 40:26 41:27 42:28 43:29 44:30 45:31 46:32 47:33 48:34 49:35 50:35 51:35 52:36 53:37 54:38 55:39 56:39 57:39 58:40 59:41 60:42 61:43 62:44 63:45 64:46 65:46 66:47 67:48 68:49 69:50 70:51 71:52 72:53 73:53 74:54 75:54 76:55 77:56 78:56 79:56 80:57 81:58 82:59 83:60 84:61 85:62 86:63 87:64 88:65 89:66 90:66 91:67 92:67 93:68 94:69 95:70 96:71 97:72 98:73 99:74 100:74 101:75 102:76 103:77 104:78 105:79 106:79 107:80 108:80 109:81 110:82 111:83 112:83 113:83 114:84 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:100 134:100 135:101 136:101 137:102 138:103 139:104 140:105 141:106 142:107 143:108 144:109 145:110 146:110 147:111 148:112 149:112 150:112 151:112 152:113 153:114 154:115 155:116 156:117 157:118 158:119 159:120 160:121 161:122 162:122 163:122 164:123 165:123
I0226 10:03:48.601650 140051752298304 squad_lib.py:366] token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True
I0226 10:03:48.601729 140051752298304 squad_lib.py:368] input_ids: 101 2054 2003 1996 10511 2460 2005 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.601802 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.601896 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.604074 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.604126 140051752298304 squad_lib.py:354] unique_id: 1000000008
I0226 10:03:48.604159 140051752298304 squad_lib.py:355] example_index: 8
I0226 10:03:48.604187 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.604244 140051752298304 squad_lib.py:358] tokens: [CLS] what was the theme of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.604304 140051752298304 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123
I0226 10:03:48.604360 140051752298304 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True
I0226 10:03:48.604436 140051752298304 squad_lib.py:368] input_ids: 101 2054 2001 1996 4323 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.604508 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.604578 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.606734 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.606788 140051752298304 squad_lib.py:354] unique_id: 1000000009
I0226 10:03:48.606822 140051752298304 squad_lib.py:355] example_index: 9
I0226 10:03:48.606850 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.606910 140051752298304 squad_lib.py:358] tokens: [CLS] what does afc stand for ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.606970 140051752298304 squad_lib.py:361] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:13 22:14 23:15 24:16 25:17 26:17 27:17 28:18 29:19 30:20 31:21 32:21 33:22 34:23 35:24 36:25 37:26 38:26 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:35 50:35 51:36 52:37 53:38 54:39 55:39 56:39 57:40 58:41 59:42 60:43 61:44 62:45 63:46 64:46 65:47 66:48 67:49 68:50 69:51 70:52 71:53 72:53 73:54 74:54 75:55 76:56 77:56 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:66 90:67 91:67 92:68 93:69 94:70 95:71 96:72 97:73 98:74 99:74 100:75 101:76 102:77 103:78 104:79 105:79 106:80 107:80 108:81 109:82 110:83 111:83 112:83 113:84 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:110 146:111 147:112 148:112 149:112 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:122 161:122 162:122 163:123 164:123
I0226 10:03:48.607027 140051752298304 squad_lib.py:366] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True
I0226 10:03:48.607106 140051752298304 squad_lib.py:368] input_ids: 101 2054 2515 10511 3233 2005 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.607189 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.607259 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.609383 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.609435 140051752298304 squad_lib.py:354] unique_id: 1000000010
I0226 10:03:48.609468 140051752298304 squad_lib.py:355] example_index: 10
I0226 10:03:48.609496 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.609553 140051752298304 squad_lib.py:358] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.609612 140051752298304 squad_lib.py:361] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123
I0226 10:03:48.609668 140051752298304 squad_lib.py:366] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True
I0226 10:03:48.609743 140051752298304 squad_lib.py:368] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.609815 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.609909 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.612048 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.612100 140051752298304 squad_lib.py:354] unique_id: 1000000011
I0226 10:03:48.612134 140051752298304 squad_lib.py:355] example_index: 11
I0226 10:03:48.612162 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.612221 140051752298304 squad_lib.py:358] tokens: [CLS] who won super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.612292 140051752298304 squad_lib.py:361] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:13 22:14 23:15 24:16 25:17 26:17 27:17 28:18 29:19 30:20 31:21 32:21 33:22 34:23 35:24 36:25 37:26 38:26 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:35 50:35 51:36 52:37 53:38 54:39 55:39 56:39 57:40 58:41 59:42 60:43 61:44 62:45 63:46 64:46 65:47 66:48 67:49 68:50 69:51 70:52 71:53 72:53 73:54 74:54 75:55 76:56 77:56 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:66 90:67 91:67 92:68 93:69 94:70 95:71 96:72 97:73 98:74 99:74 100:75 101:76 102:77 103:78 104:79 105:79 106:80 107:80 108:81 109:82 110:83 111:83 112:83 113:84 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:110 146:111 147:112 148:112 149:112 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:122 161:122 162:122 163:123 164:123
I0226 10:03:48.612348 140051752298304 squad_lib.py:366] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True
I0226 10:03:48.612440 140051752298304 squad_lib.py:368] input_ids: 101 2040 2180 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.612514 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.612586 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.614780 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.614832 140051752298304 squad_lib.py:354] unique_id: 1000000012
I0226 10:03:48.614866 140051752298304 squad_lib.py:355] example_index: 12
I0226 10:03:48.614894 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.614953 140051752298304 squad_lib.py:358] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.615013 140051752298304 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123
I0226 10:03:48.615069 140051752298304 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True
I0226 10:03:48.615148 140051752298304 squad_lib.py:368] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.615229 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.615300 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.617434 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.617485 140051752298304 squad_lib.py:354] unique_id: 1000000013
I0226 10:03:48.617519 140051752298304 squad_lib.py:355] example_index: 13
I0226 10:03:48.617547 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.617606 140051752298304 squad_lib.py:358] tokens: [CLS] what city did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.617664 140051752298304 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123
I0226 10:03:48.617719 140051752298304 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True
I0226 10:03:48.617795 140051752298304 squad_lib.py:368] input_ids: 101 2054 2103 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.617888 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.617961 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.620176 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.620227 140051752298304 squad_lib.py:354] unique_id: 1000000014
I0226 10:03:48.620260 140051752298304 squad_lib.py:355] example_index: 14
I0226 10:03:48.620288 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.620348 140051752298304 squad_lib.py:358] tokens: [CLS] if roman nu ##meral ##s were used , what would super bowl 50 have been called ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.620407 140051752298304 squad_lib.py:361] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:17 38:17 39:18 40:19 41:20 42:21 43:21 44:22 45:23 46:24 47:25 48:26 49:26 50:26 51:27 52:28 53:29 54:30 55:31 56:32 57:33 58:34 59:35 60:35 61:35 62:36 63:37 64:38 65:39 66:39 67:39 68:40 69:41 70:42 71:43 72:44 73:45 74:46 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:53 84:54 85:54 86:55 87:56 88:56 89:56 90:57 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:66 100:66 101:67 102:67 103:68 104:69 105:70 106:71 107:72 108:73 109:74 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:80 119:81 120:82 121:83 122:83 123:83 124:84 125:84 126:85 127:86 128:87 129:88 130:89 131:89 132:90 133:91 134:92 135:93 136:94 137:95 138:96 139:97 140:98 141:99 142:100 143:100 144:100 145:101 146:101 147:102 148:103 149:104 150:105 151:106 152:107 153:108 154:109 155:110 156:110 157:111 158:112 159:112 160:112 161:112 162:113 163:114 164:115 165:116 166:117 167:118 168:119 169:120 170:121 171:122 172:122 173:122 174:123 175:123
I0226 10:03:48.620462 140051752298304 squad_lib.py:366] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True
I0226 10:03:48.620538 140051752298304 squad_lib.py:368] input_ids: 101 2065 3142 16371 28990 2015 2020 2109 1010 2054 2052 3565 4605 2753 2031 2042 2170 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.620610 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.620681 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.622880 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.622933 140051752298304 squad_lib.py:354] unique_id: 1000000015
I0226 10:03:48.622967 140051752298304 squad_lib.py:355] example_index: 15
I0226 10:03:48.622995 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.623055 140051752298304 squad_lib.py:358] tokens: [CLS] super bowl 50 decided the nfl champion for what season ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.623115 140051752298304 squad_lib.py:361] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:13 27:14 28:15 29:16 30:17 31:17 32:17 33:18 34:19 35:20 36:21 37:21 38:22 39:23 40:24 41:25 42:26 43:26 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:33 52:34 53:35 54:35 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:53 78:54 79:54 80:55 81:56 82:56 83:56 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:80 113:81 114:82 115:83 116:83 117:83 118:84 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:110 151:111 152:112 153:112 154:112 155:112 156:113 157:114 158:115 159:116 160:117 161:118 162:119 163:120 164:121 165:122 166:122 167:122 168:123 169:123
I0226 10:03:48.623182 140051752298304 squad_lib.py:366] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True
I0226 10:03:48.623258 140051752298304 squad_lib.py:368] input_ids: 101 3565 4605 2753 2787 1996 5088 3410 2005 2054 2161 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.623329 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.623400 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.625587 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.625638 140051752298304 squad_lib.py:354] unique_id: 1000000016
I0226 10:03:48.625671 140051752298304 squad_lib.py:355] example_index: 16
I0226 10:03:48.625698 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.625757 140051752298304 squad_lib.py:358] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.625816 140051752298304 squad_lib.py:361] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123
I0226 10:03:48.625893 140051752298304 squad_lib.py:366] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True
I0226 10:03:48.625973 140051752298304 squad_lib.py:368] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.626045 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.626118 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.628294 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.628347 140051752298304 squad_lib.py:354] unique_id: 1000000017
I0226 10:03:48.628381 140051752298304 squad_lib.py:355] example_index: 17
I0226 10:03:48.628409 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.628467 140051752298304 squad_lib.py:358] tokens: [CLS] what city did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.628525 140051752298304 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123
I0226 10:03:48.628581 140051752298304 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True
I0226 10:03:48.628657 140051752298304 squad_lib.py:368] input_ids: 101 2054 2103 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.628728 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.628798 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.631020 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.631073 140051752298304 squad_lib.py:354] unique_id: 1000000018
I0226 10:03:48.631108 140051752298304 squad_lib.py:355] example_index: 18
I0226 10:03:48.631136 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.631204 140051752298304 squad_lib.py:358] tokens: [CLS] what stadium did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.631263 140051752298304 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123
I0226 10:03:48.631318 140051752298304 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True
I0226 10:03:48.631395 140051752298304 squad_lib.py:368] input_ids: 101 2054 3346 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.631465 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.631536 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.633665 140051752298304 squad_lib.py:353] *** Example ***
I0226 10:03:48.633716 140051752298304 squad_lib.py:354] unique_id: 1000000019
I0226 10:03:48.633750 140051752298304 squad_lib.py:355] example_index: 19
I0226 10:03:48.633777 140051752298304 squad_lib.py:356] doc_span_index: 0
I0226 10:03:48.633847 140051752298304 squad_lib.py:358] tokens: [CLS] what was the final score of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24  10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the " golden anniversary " with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as " super bowl l " ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]
I0226 10:03:48.633924 140051752298304 squad_lib.py:361] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123
I0226 10:03:48.633981 140051752298304 squad_lib.py:366] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True
I0226 10:03:48.634060 140051752298304 squad_lib.py:368] input_ids: 101 2054 2001 1996 2345 3556 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.634132 140051752298304 squad_lib.py:369] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:03:48.634205 140051752298304 squad_lib.py:370] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0226 10:04:11.290028 140051752298304 squad_lib.py:407] Adding padding examples to make sure no partial batch.
I0226 10:04:11.290144 140051752298304 squad_lib.py:408] Adds 3 padding examples for inference.
I0226 10:04:11.291627 140051752298304 run_squad.py:350] ***** Running predictions *****
I0226 10:04:11.291694 140051752298304 run_squad.py:351]   Num orig examples = 10570
I0226 10:04:11.291728 140051752298304 run_squad.py:352]   Num split examples = 10833
I0226 10:04:11.291761 140051752298304 run_squad.py:353]   Batch size = 4
WARNING:tensorflow:BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler_1" was not an Input tensor, it was generated by layer input_mask.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_mask_1:0
W0226 10:04:15.391235 140051752298304 network.py:1344] BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler_1" was not an Input tensor, it was generated by layer input_mask.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_mask_1:0
WARNING:tensorflow:BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler_1" was not an Input tensor, it was generated by layer input_type_ids.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_type_ids_1:0
W0226 10:04:15.391365 140051752298304 network.py:1344] BertSpanLabeler inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to "bert_span_labeler_1" was not an Input tensor, it was generated by layer input_type_ids.
Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.
The tensor that caused the issue was: input_type_ids_1:0
I0226 10:04:15.399576 140051752298304 run_squad.py:193] Restoring checkpoints from /nfs/site/home/jojimonv/Models/TF2.0/BERT/models/squad_model_dir/ctl_step_5540.ckpt-2
I0226 10:05:42.493961 140051752298304 run_squad.py:221] Made predictions for 100 records.
I0226 10:07:06.559740 140051752298304 run_squad.py:221] Made predictions for 200 records.
I0226 10:08:30.635622 140051752298304 run_squad.py:221] Made predictions for 300 records.
I0226 10:09:54.488605 140051752298304 run_squad.py:221] Made predictions for 400 records.
I0226 10:11:18.330440 140051752298304 run_squad.py:221] Made predictions for 500 records.
I0226 10:12:43.115637 140051752298304 run_squad.py:221] Made predictions for 600 records.
I0226 10:14:06.295169 140051752298304 run_squad.py:221] Made predictions for 700 records.
I0226 10:15:31.652915 140051752298304 run_squad.py:221] Made predictions for 800 records.
I0226 10:16:56.790924 140051752298304 run_squad.py:221] Made predictions for 900 records.
I0226 10:18:21.305050 140051752298304 run_squad.py:221] Made predictions for 1000 records.
I0226 10:19:45.587647 140051752298304 run_squad.py:221] Made predictions for 1100 records.
I0226 10:21:09.206982 140051752298304 run_squad.py:221] Made predictions for 1200 records.
I0226 10:22:33.086531 140051752298304 run_squad.py:221] Made predictions for 1300 records.
I0226 10:23:57.827390 140051752298304 run_squad.py:221] Made predictions for 1400 records.
I0226 10:25:21.429269 140051752298304 run_squad.py:221] Made predictions for 1500 records.
I0226 10:26:46.114998 140051752298304 run_squad.py:221] Made predictions for 1600 records.
I0226 10:28:10.387719 140051752298304 run_squad.py:221] Made predictions for 1700 records.
I0226 10:29:35.227453 140051752298304 run_squad.py:221] Made predictions for 1800 records.
I0226 10:31:00.268038 140051752298304 run_squad.py:221] Made predictions for 1900 records.
I0226 10:32:23.538501 140051752298304 run_squad.py:221] Made predictions for 2000 records.
I0226 10:33:49.508115 140051752298304 run_squad.py:221] Made predictions for 2100 records.
I0226 10:35:12.383610 140051752298304 run_squad.py:221] Made predictions for 2200 records.
I0226 10:36:36.439902 140051752298304 run_squad.py:221] Made predictions for 2300 records.
I0226 10:38:01.258240 140051752298304 run_squad.py:221] Made predictions for 2400 records.
I0226 10:39:25.117673 140051752298304 run_squad.py:221] Made predictions for 2500 records.
I0226 10:40:48.051257 140051752298304 run_squad.py:221] Made predictions for 2600 records.
I0226 10:42:12.088344 140051752298304 run_squad.py:221] Made predictions for 2700 records.
I0226 10:43:37.715844 140051752298304 run_squad.py:221] Made predictions for 2800 records.
I0226 10:45:03.079617 140051752298304 run_squad.py:221] Made predictions for 2900 records.
I0226 10:46:28.699659 140051752298304 run_squad.py:221] Made predictions for 3000 records.
I0226 10:47:52.902692 140051752298304 run_squad.py:221] Made predictions for 3100 records.
I0226 10:49:17.942274 140051752298304 run_squad.py:221] Made predictions for 3200 records.
I0226 10:50:42.101975 140051752298304 run_squad.py:221] Made predictions for 3300 records.
I0226 10:52:04.832735 140051752298304 run_squad.py:221] Made predictions for 3400 records.
I0226 10:53:29.945452 140051752298304 run_squad.py:221] Made predictions for 3500 records.
I0226 10:54:54.888180 140051752298304 run_squad.py:221] Made predictions for 3600 records.
I0226 10:56:19.643093 140051752298304 run_squad.py:221] Made predictions for 3700 records.
I0226 10:57:44.763371 140051752298304 run_squad.py:221] Made predictions for 3800 records.
I0226 10:59:07.313318 140051752298304 run_squad.py:221] Made predictions for 3900 records.
I0226 11:00:30.642605 140051752298304 run_squad.py:221] Made predictions for 4000 records.
I0226 11:01:53.319510 140051752298304 run_squad.py:221] Made predictions for 4100 records.
I0226 11:03:15.878704 140051752298304 run_squad.py:221] Made predictions for 4200 records.
I0226 11:04:40.444689 140051752298304 run_squad.py:221] Made predictions for 4300 records.
I0226 11:06:05.499091 140051752298304 run_squad.py:221] Made predictions for 4400 records.
I0226 11:07:28.902924 140051752298304 run_squad.py:221] Made predictions for 4500 records.
I0226 11:08:51.524274 140051752298304 run_squad.py:221] Made predictions for 4600 records.
I0226 11:10:15.197487 140051752298304 run_squad.py:221] Made predictions for 4700 records.
I0226 11:11:37.712280 140051752298304 run_squad.py:221] Made predictions for 4800 records.
I0226 11:13:00.366833 140051752298304 run_squad.py:221] Made predictions for 4900 records.
I0226 11:14:26.318282 140051752298304 run_squad.py:221] Made predictions for 5000 records.
I0226 11:15:51.858041 140051752298304 run_squad.py:221] Made predictions for 5100 records.
I0226 11:17:16.598735 140051752298304 run_squad.py:221] Made predictions for 5200 records.
I0226 11:18:41.519204 140051752298304 run_squad.py:221] Made predictions for 5300 records.
I0226 11:20:06.978274 140051752298304 run_squad.py:221] Made predictions for 5400 records.
I0226 11:21:31.792829 140051752298304 run_squad.py:221] Made predictions for 5500 records.
I0226 11:22:56.758020 140051752298304 run_squad.py:221] Made predictions for 5600 records.
I0226 11:24:21.804280 140051752298304 run_squad.py:221] Made predictions for 5700 records.
I0226 11:25:47.343163 140051752298304 run_squad.py:221] Made predictions for 5800 records.
I0226 11:27:11.856068 140051752298304 run_squad.py:221] Made predictions for 5900 records.
I0226 11:28:36.513792 140051752298304 run_squad.py:221] Made predictions for 6000 records.
I0226 11:30:00.978936 140051752298304 run_squad.py:221] Made predictions for 6100 records.
I0226 11:31:24.638516 140051752298304 run_squad.py:221] Made predictions for 6200 records.
I0226 11:32:48.908294 140051752298304 run_squad.py:221] Made predictions for 6300 records.
I0226 11:34:11.799012 140051752298304 run_squad.py:221] Made predictions for 6400 records.
I0226 11:35:34.280886 140051752298304 run_squad.py:221] Made predictions for 6500 records.
I0226 11:36:59.009478 140051752298304 run_squad.py:221] Made predictions for 6600 records.
I0226 11:38:24.111886 140051752298304 run_squad.py:221] Made predictions for 6700 records.
I0226 11:39:46.764477 140051752298304 run_squad.py:221] Made predictions for 6800 records.
I0226 11:41:10.098171 140051752298304 run_squad.py:221] Made predictions for 6900 records.
I0226 11:42:35.329713 140051752298304 run_squad.py:221] Made predictions for 7000 records.
I0226 11:43:58.960663 140051752298304 run_squad.py:221] Made predictions for 7100 records.
I0226 11:45:23.533664 140051752298304 run_squad.py:221] Made predictions for 7200 records.
I0226 11:46:47.239441 140051752298304 run_squad.py:221] Made predictions for 7300 records.
I0226 11:48:11.803944 140051752298304 run_squad.py:221] Made predictions for 7400 records.
I0226 11:49:35.266410 140051752298304 run_squad.py:221] Made predictions for 7500 records.
I0226 11:50:59.098119 140051752298304 run_squad.py:221] Made predictions for 7600 records.
I0226 11:52:21.786964 140051752298304 run_squad.py:221] Made predictions for 7700 records.
I0226 11:53:44.540263 140051752298304 run_squad.py:221] Made predictions for 7800 records.
I0226 11:55:07.091031 140051752298304 run_squad.py:221] Made predictions for 7900 records.
I0226 11:56:30.909967 140051752298304 run_squad.py:221] Made predictions for 8000 records.
I0226 11:57:55.565924 140051752298304 run_squad.py:221] Made predictions for 8100 records.
I0226 11:59:19.981489 140051752298304 run_squad.py:221] Made predictions for 8200 records.
I0226 12:00:43.249840 140051752298304 run_squad.py:221] Made predictions for 8300 records.
I0226 12:02:05.232693 140051752298304 run_squad.py:221] Made predictions for 8400 records.
I0226 12:03:28.312676 140051752298304 run_squad.py:221] Made predictions for 8500 records.
I0226 12:04:51.793897 140051752298304 run_squad.py:221] Made predictions for 8600 records.
I0226 12:06:15.109073 140051752298304 run_squad.py:221] Made predictions for 8700 records.
I0226 12:07:39.201160 140051752298304 run_squad.py:221] Made predictions for 8800 records.
I0226 12:09:03.923735 140051752298304 run_squad.py:221] Made predictions for 8900 records.
I0226 12:10:28.626942 140051752298304 run_squad.py:221] Made predictions for 9000 records.
I0226 12:11:53.700305 140051752298304 run_squad.py:221] Made predictions for 9100 records.
I0226 12:13:17.546367 140051752298304 run_squad.py:221] Made predictions for 9200 records.
I0226 12:14:41.095208 140051752298304 run_squad.py:221] Made predictions for 9300 records.
I0226 12:16:04.570238 140051752298304 run_squad.py:221] Made predictions for 9400 records.
I0226 12:17:29.848124 140051752298304 run_squad.py:221] Made predictions for 9500 records.
I0226 12:18:55.117061 140051752298304 run_squad.py:221] Made predictions for 9600 records.
I0226 12:20:20.104057 140051752298304 run_squad.py:221] Made predictions for 9700 records.
I0226 12:21:44.401625 140051752298304 run_squad.py:221] Made predictions for 9800 records.
I0226 12:23:09.812349 140051752298304 run_squad.py:221] Made predictions for 9900 records.
I0226 12:24:34.058978 140051752298304 run_squad.py:221] Made predictions for 10000 records.
I0226 12:25:58.390426 140051752298304 run_squad.py:221] Made predictions for 10100 records.
I0226 12:27:23.955994 140051752298304 run_squad.py:221] Made predictions for 10200 records.
I0226 12:28:48.907212 140051752298304 run_squad.py:221] Made predictions for 10300 records.
I0226 12:30:13.092467 140051752298304 run_squad.py:221] Made predictions for 10400 records.
I0226 12:31:36.380053 140051752298304 run_squad.py:221] Made predictions for 10500 records.
I0226 12:33:01.257365 140051752298304 run_squad.py:221] Made predictions for 10600 records.
I0226 12:34:26.477983 140051752298304 run_squad.py:221] Made predictions for 10700 records.
I0226 12:35:51.194852 140051752298304 run_squad.py:221] Made predictions for 10800 records.
I0226 12:36:22.435065 140051752298304 squad_lib.py:510] Writing predictions to: /nfs/site/home/jojimonv/Models/TF2.0/BERT/models/squad_model_dir/predictions.json
I0226 12:36:22.435240 140051752298304 squad_lib.py:511] Writing nbest to: /nfs/site/home/jojimonv/Models/TF2.0/BERT/models/squad_model_dir/nbest_predictions.json
